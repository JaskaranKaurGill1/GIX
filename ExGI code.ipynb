{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvHTKTtPiEPA"
      },
      "source": [
        "# ExGI - Gene Interaction Extraction algorithm\n",
        "\n",
        " The following two cell imports the needed packages, pretrained BioBERT weights and loads pretrained BioBert model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxP3wpXGNCbe",
        "outputId": "0ce11935-766d-4865-8acd-cea82bc59c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.1-py3-none-any.whl (492 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.4/492.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.14.1 dill-0.3.7 huggingface-hub-0.16.4 multiprocess-0.70.15 xxhash-3.3.0\n",
            "--2023-07-30 15:04:43--  http://nlp.dmis.korea.edu/projects/biobert-2020-checkpoints/biobert_v1.1_pubmed.tar.gz\n",
            "Resolving nlp.dmis.korea.edu (nlp.dmis.korea.edu)... 163.152.163.168\n",
            "Connecting to nlp.dmis.korea.edu (nlp.dmis.korea.edu)|163.152.163.168|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 401403346 (383M) [application/x-gzip]\n",
            "Saving to: ‘biobert_v1.1_pubmed.tar.gz’\n",
            "\n",
            "biobert_v1.1_pubmed 100%[===================>] 382.81M  3.32MB/s    in 2m 0s   \n",
            "\n",
            "2023-07-30 15:06:45 (3.18 MB/s) - ‘biobert_v1.1_pubmed.tar.gz’ saved [401403346/401403346]\n",
            "\n",
            "biobert_v1.1_pubmed/\n",
            "biobert_v1.1_pubmed/model.ckpt-1000000.data-00000-of-00001\n",
            "biobert_v1.1_pubmed/model.ckpt-1000000.meta\n",
            "biobert_v1.1_pubmed/bert_config.json\n",
            "biobert_v1.1_pubmed/vocab.txt\n",
            "biobert_v1.1_pubmed/model.ckpt-1000000.index\n",
            "Collecting pytorch_transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (1.22.4)\n",
            "Collecting boto3 (from pytorch_transformers)\n",
            "  Downloading boto3-1.28.15-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (4.65.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2022.10.31)\n",
            "Collecting sentencepiece (from pytorch_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses (from pytorch_transformers)\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->pytorch_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->pytorch_transformers) (16.0.6)\n",
            "Collecting botocore<1.32.0,>=1.31.15 (from boto3->pytorch_transformers)\n",
            "  Downloading botocore-1.31.15-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_transformers)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->pytorch_transformers)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch_transformers) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch_transformers) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch_transformers) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.15->boto3->pytorch_transformers) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch_transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895239 sha256=1e7628f4035d9cad10efe0cd9ac15a2b1221f183f283194e10b9fd19b839d709\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, jmespath, botocore, s3transfer, boto3, pytorch_transformers\n",
            "Successfully installed boto3-1.28.15 botocore-1.31.15 jmespath-1.0.1 pytorch_transformers-1.2.0 s3transfer-0.6.1 sacremoses-0.0.53 sentencepiece-0.1.99\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, transformers\n",
            "Successfully installed safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n",
            "tar (child): biobert_weights: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n",
            "bert_config.json\t\t\tmodel.ckpt-1000000.index  vocab.txt\n",
            "model.ckpt-1000000.data-00000-of-00001\tmodel.ckpt-1000000.meta\n",
            "2023-07-30 15:07:11.036118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Building PyTorch model from configuration: BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Converting TensorFlow checkpoint from /content/biobert_v1.1_pubmed/model.ckpt-1000000\n",
            "Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
            "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "Loading TF weight bert/embeddings/word_embeddings with shape [28996, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "Loading TF weight bert/pooler/dense/bias with shape [768]\n",
            "Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "Save PyTorch model to biobert_v1.1_pubmed/pytorch_model.bin\n",
            "bert_config.json\t\t\tmodel.ckpt-1000000.meta\n",
            "model.ckpt-1000000.data-00000-of-00001\tpytorch_model.bin\n",
            "model.ckpt-1000000.index\t\tvocab.txt\n",
            "config.json\t\t\t\tmodel.ckpt-1000000.meta\n",
            "model.ckpt-1000000.data-00000-of-00001\tpytorch_model.bin\n",
            "model.ckpt-1000000.index\t\tvocab.txt\n",
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-nlp\n",
            "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.22.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.28.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (4.65.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (16.0.6)\n",
            "Requirement already satisfied: botocore<1.32.0,>=1.31.15 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (1.31.15)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (0.6.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.15->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.15->boto3->pytorch_pretrained_bert) (1.16.0)\n",
            "Installing collected packages: pytorch-nlp, pytorch_pretrained_bert\n",
            "Successfully installed pytorch-nlp-0.5.0 pytorch_pretrained_bert-0.6.2\n",
            "Collecting Keras-Preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.16.0)\n",
            "Installing collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.1.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install datasets\n",
        "\n",
        "!wget http://nlp.dmis.korea.edu/projects/biobert-2020-checkpoints/biobert_v1.1_pubmed.tar.gz\n",
        "!tar -xvzf /content/biobert_v1.1_pubmed.tar.gz\n",
        "\n",
        "!pip install pytorch_transformers\n",
        "!pip install transformers\n",
        "!tar -xzf biobert_weights\n",
        "!ls biobert_v1.1_pubmed/\n",
        "!transformers-cli convert --model_type bert --tf_checkpoint biobert_v1.1_pubmed/model.ckpt-1000000 --config biobert_v1.1_pubmed/bert_config.json --pytorch_dump_output biobert_v1.1_pubmed/pytorch_model.bin\n",
        "!ls biobert_v1.1_pubmed/\n",
        "!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json\n",
        "!ls biobert_v1.1_pubmed/\n",
        "from pytorch_transformers import BertModel\n",
        "DATA_DIR=\".\"\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# install BERT\n",
        "!pip install pytorch_pretrained_bert pytorch-nlp\n",
        "\n",
        "# BERT imports\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "!pip install Keras-Preprocessing\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#% matplotlib inline\n",
        "\n",
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n",
        "model = BertForSequenceClassification.from_pretrained(\"biobert_v1.1_pubmed\", num_labels=2)#binary classification\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBAh45KRr131"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWIEdpHiiBOC",
        "outputId": "1b824a82-a550-43d5-ce7b-b972dd68605c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/3.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m2.2/3.1 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.22.4)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.81\n",
            "Collecting Bio\n",
            "  Downloading bio-1.5.9-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: biopython>=1.80 in /usr/local/lib/python3.10/dist-packages (from Bio) (1.81)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Bio) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from Bio) (4.65.0)\n",
            "Collecting mygene (from Bio)\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from Bio) (1.5.3)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from Bio) (1.6.0)\n",
            "Collecting gprofiler-official (from Bio)\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython>=1.80->Bio) (1.22.4)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->Bio)\n",
            "  Downloading biothings_client-0.3.0-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2022.7.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->Bio) (1.16.0)\n",
            "Installing collected packages: gprofiler-official, biothings-client, mygene, Bio\n",
            "Successfully installed Bio-1.5.9 biothings-client-0.3.0 gprofiler-official-1.0.0 mygene-3.2.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bs4\n",
        "from urllib.request import urlopen as uReq\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from openpyxl import load_workbook\n",
        "!pip install biopython\n",
        "!pip install Bio\n",
        "\n",
        "from Bio import Entrez\n",
        "import nltk.data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import requests\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oGvtDTzi9ac"
      },
      "source": [
        "# **Pre-processing - Search query with target specific keywords and retrieve relevant abstracts:** The following cell contains the methods to fetch literature abstracts as per selected keywords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAodqCsSLUAj"
      },
      "outputs": [],
      "source": [
        "#Abstract Fetch\n",
        "def search(query,amt):\n",
        "    Entrez.email = 'jassig2905@GMAIL.COM'\n",
        "    handle = Entrez.esearch(db='pubmed',\n",
        "                            sort='relevance',\n",
        "                            retmax=amt,\n",
        "                            retmode='xml',\n",
        "                            term=query)\n",
        "    results = Entrez.read(handle)\n",
        "    return results\n",
        "\n",
        "def make_a_link(id):\n",
        "    return(\"https://pubmed.ncbi.nlm.nih.gov/\"+str(id)+\"/\")\n",
        "\n",
        "def extract_articles(list_of_ids):\n",
        "    Article_title = []\n",
        "    Article_Abstract = []\n",
        "    Pub_med_link = []\n",
        "    Link_full_articles = []\n",
        "    Type_of_articles = []\n",
        "    for id in list_of_ids:\n",
        "        uClient  = uReq(make_a_link(id))\n",
        "        page_html = uClient.read()\n",
        "        page_soup = soup(page_html, \"html.parser\")\n",
        "        Title = page_soup.find(\"h1\",{\"class\":\"heading-title\"}).text.replace(\"\\n\",\"\").strip()\n",
        "\n",
        "        Abstract = page_soup.find(\"div\",{\"class\":\"abstract-content\"})\n",
        "        if(Abstract is not None):\n",
        "            Abstract = Abstract.text.replace(\"\\n\",\"\").strip()\n",
        "        #print(page_soup.find(\"div\",{'class':'action-buttons inline'}))\n",
        "        Full_Article_link = page_soup.find(\"a\",{\"data-ga-action\":\"DOI\"})\n",
        "        if(Full_Article_link is not None):\n",
        "            Full_Article_link = page_soup.find(\"a\",{\"data-ga-action\":\"DOI\"})[\"href\"]\n",
        "            Link_full_articles.append(Full_Article_link)\n",
        "        else:\n",
        "            Link_full_articles.append(\"NA\")\n",
        "        Type = page_soup.find(\"span\",\"article-source\").text.strip()\n",
        "        Article_title.append(Title)\n",
        "        Article_Abstract.append(Abstract)\n",
        "        Pub_med_link.append(make_a_link(id))\n",
        "        #Link_full_articles.append(Full_Article_link)\n",
        "        Type_of_articles.append(Type)\n",
        "    new_articles = pd.DataFrame()\n",
        "    new_articles[\"Title\"]=Article_title\n",
        "    new_articles[\"abstract\"]=Article_Abstract\n",
        "    new_articles[\"link to full article\"]=Link_full_articles\n",
        "    new_articles['Type']=Type_of_articles\n",
        "    new_articles[\"pub_med_links\"]=Pub_med_link\n",
        "    return(new_articles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St-b4WeOjYBD"
      },
      "source": [
        "# **Pre-processing - Sentence Tokenisation**: The following cell contains a method to tokenize abstracts into sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0UmdNPoLq0o"
      },
      "outputs": [],
      "source": [
        "#chunk into Sentences\n",
        "\n",
        "def tokenize_abs(df):\n",
        "  tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "  ids = []\n",
        "  sents = []\n",
        "  for x in range(len(df)):\n",
        "      data = df.iloc[x]['abstract']\n",
        "      sentences = tokenizer.tokenize(data)\n",
        "      for sent in sentences:\n",
        "          sents.append(sent)\n",
        "          ids.append(x)\n",
        "  sentences_df = pd.DataFrame()\n",
        "  sentences_df['id']=ids\n",
        "  sentences_df['Sentence']=sents\n",
        "  return(sentences_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVZdMm2m9FB8"
      },
      "outputs": [],
      "source": [
        "def filter_sentences_genes(df,list_of_genes):\n",
        "\n",
        "  contains_ = []\n",
        "  gene = []\n",
        "  for x in range(len(df)):\n",
        "    c = 0\n",
        "    g = ''\n",
        "    sent = df['Sentence'].iloc[x]\n",
        "    for y in range(len(list_of_genes)):\n",
        "      if (list_of_genes[y] in sent):\n",
        "        c = 1\n",
        "        g = list_of_genes[y]\n",
        "        continue\n",
        "    contains_.append(c)\n",
        "    gene.append(g)\n",
        "  df['Contain']=contains_\n",
        "  df['Gene'] = gene\n",
        "  return(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS6htj3GjkJa"
      },
      "source": [
        "# **Pre-processing - Sentence Eliminator 1:** The following cell contains the method that eliminate sentences unrelated to gene regulation or expression. The method uses our pretrained classifier sentence_extraction_f.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrDth-IgNKpR"
      },
      "outputs": [],
      "source": [
        "#Sentence classifier 1\n",
        "def sentence_classification_1(df, max_len):\n",
        "  model.load_state_dict(torch.load('drive/MyDrive/E. Coli/sentence_extraction_f.pth'))\n",
        "  model.eval()\n",
        "  Sentences = df['Sentence']\n",
        "  tokenizer = BertTokenizer.from_pretrained('biobert_v1.1_pubmed', do_lower_case=True)\n",
        "\n",
        "  tokenized_texts = list(map(lambda t: ['[CLS]']+tokenizer.tokenize(t)+['[SEP]'] , Sentences))\n",
        "  #print (\"Tokenize the first sentence:\")\n",
        "  max_len = max_len\n",
        "  input_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, tokenized_texts)),\n",
        "                            maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "  input_ids = pad_sequences(input_ids, maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  attention_masks = []\n",
        "\n",
        "  for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "  batch_size = 8\n",
        "  validation_inputs = torch.tensor(input_ids)\n",
        "  validation_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
        "\n",
        "  # Create an iterator of our data with torch DataLoader\n",
        "\n",
        "  validation_data = TensorDataset(validation_inputs, validation_masks)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "  all_predictions=[]\n",
        "  all_predictions_0 = []\n",
        "  all_predictions_1= []\n",
        "  #true_labels = []\n",
        "  logits_abc = []\n",
        "  for batch in validation_dataloader:\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask = batch\n",
        "      # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "      with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        #label_ids = b_labels.to('cpu').numpy()\n",
        "        predicted_class = []\n",
        "        logits_abc.append(logits)\n",
        "        for x in range(len(logits)):\n",
        "          if(logits[x][0]>0):\n",
        "            abc=0\n",
        "          else:\n",
        "            abc=1\n",
        "          all_predictions.append(abc)\n",
        "        #tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        logits = logits.cpu()\n",
        "        logits = (tf.nn.sigmoid(logits).numpy())\n",
        "        #logits = logits.detach().cpu().numpy()\n",
        "        for x in range(len(logits)):\n",
        "          all_predictions_0.append(logits[x][0])\n",
        "          all_predictions_1.append(logits[x][1])\n",
        "          #true_labels.append(label_ids[x])\n",
        "  df['predictied']=all_predictions\n",
        "  df['prediction_0']=all_predictions_0\n",
        "  df['prediction_1']=all_predictions_1\n",
        "  print(\"Sentence Eliminator 1: eliminated\", len(df)-np.sum(df['predictied']), \"out of \",len(df))\n",
        "  df = df.fillna('')\n",
        "  return(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SDDD620kH6A"
      },
      "source": [
        "# **Pre-processing: Sentence Eliminator 2** The following cell contains the method that eliminate sentences that do not mention any gene/protein entities. The method uses an NER pretrained model BERN2.0.\n",
        "\n",
        "BERN2.0 web service call may disconnect at times due to exceeding connection calls. We recommend running the program in small batches for bigger datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gPg9MRIQ7ri"
      },
      "outputs": [],
      "source": [
        "#Sentence classifier 2\n",
        "\n",
        "\n",
        "def return_genes(sentence,X):\n",
        "    genes = []\n",
        "\n",
        "    for x in range(len(X['annotations'])):\n",
        "        typ = X['annotations'][x]['obj']\n",
        "        if(typ == \"gene\"):\n",
        "            begin = X['annotations'][x]['span']['begin']\n",
        "            end = X['annotations'][x]['span']['end']\n",
        "            sent = sentence\n",
        "            genes.append(sent[begin:end])\n",
        "    return(genes)\n",
        "\n",
        "def if_contains_genes(df):\n",
        "  target_gene = []\n",
        "  all_genes = []\n",
        "  for pos in range(len(df)):\n",
        "\n",
        "      RET = query_raw(df['Sentence'].iloc[pos])\n",
        "      all_genes.append(return_genes(df['Sentence'].iloc[pos],RET))\n",
        "      #print(pos,df['Sentence'].iloc[pos])\n",
        "      objs = [RET['annotations'][x]['obj'] for x in range(len(RET['annotations']))].count('gene')\n",
        "      if(objs>1):\n",
        "          target_gene.append(1)\n",
        "      else:\n",
        "          target_gene.append(0)\n",
        "  df['gene_mention']=target_gene\n",
        "  df['All_X']= all_genes\n",
        "  print(\"Sentence Eliminator 2: eliminated\", len(df)-np.sum(df['gene_mention']), \"out of \",len(df))\n",
        "  df = df.fillna('')\n",
        "  return(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZXCcra2ksgb"
      },
      "source": [
        "# **Relation Extraction: Named entity recognition and entity tagging** The following cell contains the NER method to identify the gene/protein entities and tag sentences with all possible pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuVR1OEXWcjM"
      },
      "outputs": [],
      "source": [
        "#NER\n",
        "\n",
        "def return_gene_pairs(sentence,X):\n",
        "    genes = []\n",
        "\n",
        "    for x in range(len(X['annotations'])):\n",
        "        typ = X['annotations'][x]['obj']\n",
        "        if(typ == \"gene\"):\n",
        "            begin = X['annotations'][x]['span']['begin']\n",
        "            end = X['annotations'][x]['span']['end']\n",
        "            sent = sentence\n",
        "            genes.append(sent[begin:end])\n",
        "\n",
        "    gene_pairs = []\n",
        "    for row in genes:\n",
        "        for column in genes:\n",
        "            gene_pairs.append([row,column])\n",
        "\n",
        "    return(gene_pairs)\n",
        "\n",
        "def return_gene_pos(sentence,X):\n",
        "    pos = []\n",
        "\n",
        "    for x in range(len(X['annotations'])):\n",
        "        typ = X['annotations'][x]['obj']\n",
        "        if(typ == \"gene\"):\n",
        "            begin = X['annotations'][x]['span']['begin']\n",
        "            end = X['annotations'][x]['span']['end']\n",
        "            sent = sentence\n",
        "            pos.append([begin,end])\n",
        "    pos = check_if_operon(sentence,pos)\n",
        "    return(pos)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def return_gene_pos_pairs(sentence,X):\n",
        "    pos = []\n",
        "\n",
        "    for x in range(len(X['annotations'])):\n",
        "        typ = X['annotations'][x]['obj']\n",
        "        if(typ == \"gene\"):\n",
        "            begin = X['annotations'][x]['span']['begin']\n",
        "            end = X['annotations'][x]['span']['end']\n",
        "            sent = sentence\n",
        "            pos.append([begin,end])\n",
        "    pos = check_if_operon(sentence,pos)\n",
        "    gene_pos_pairs = []\n",
        "    for row in pos:\n",
        "        for column in pos:\n",
        "            gene_pos_pairs.append([row,column])\n",
        "    return(gene_pos_pairs)\n",
        "\n",
        "\n",
        "def query_raw(text, url=\"http://bern2.korea.ac.kr/plain\"):\n",
        "  try:\n",
        "    return requests.post(url, json={'text': text}).json()\n",
        "  except:\n",
        "    query_raw(text)\n",
        "\n",
        "\n",
        "\n",
        "def check_if_operon(sentence,pos):\n",
        "    new_list = pos.copy()\n",
        "    pos_to_remove = []\n",
        "    for x in range(len(pos)-1):\n",
        "\n",
        "        #print(len(pos))\n",
        "        #print(x,'----------------------')\n",
        "        this = pos[x][1]\n",
        "        next = pos[x+1][0]\n",
        "        if(next-this==1):\n",
        "            if(sentence[next-1]==\"-\"):\n",
        "                pos_to_remove.append(x)\n",
        "                pos_to_remove.append(x+1)\n",
        "                #new_list.remove(pos[x])\n",
        "                #new_list.remove(pos[x+1])\n",
        "                #new_list.insert(x,[pos[x][0],pos[x+1][1]])\n",
        "    #print(new_list)\n",
        "    #print(pos)\n",
        "    #print(pos_to_remove)\n",
        "    if(len(pos_to_remove)>0):\n",
        "        pos_to_remove = list(dict.fromkeys(pos_to_remove))\n",
        "        pair_start = pos_to_remove[0]\n",
        "        pair_end = 0\n",
        "        #print(pos_to_remove)\n",
        "        for y in pos_to_remove:\n",
        "            new_list.remove(pos[y])\n",
        "\n",
        "\n",
        "        for y in range(len(pos_to_remove)):\n",
        "\n",
        "            if(y != len(pos_to_remove)-1):\n",
        "                #print(pos_to_remove[y],pos_to_remove[y+1],pos_to_remove[y]-pos_to_remove[y+1])\n",
        "                if(pos_to_remove[y]-pos_to_remove[y+1] == -1):\n",
        "                    pair_end = 0\n",
        "                    #print(\"here\")\n",
        "                else:\n",
        "                    pair_end = pos_to_remove[y]\n",
        "                if(pair_end != 0):\n",
        "                    new_list.insert(pair_start,[pos[pair_start][0],pos[pair_end][1]])\n",
        "                    pair_end = 0\n",
        "                    pair_start = pos_to_remove[y+1]\n",
        "            else:\n",
        "                pair_end = pos_to_remove[y]\n",
        "                new_list.insert(pair_start,[pos[pair_start][0],pos[pair_end][1]])\n",
        "\n",
        "    return(new_list)\n",
        "\n",
        "\n",
        "def NER_1(df):\n",
        "  #print(len(df))\n",
        "  ID = []\n",
        "  sent_tagged = []\n",
        "  sent_original = []\n",
        "  pair = []\n",
        "  add_lengths = 0\n",
        "  agents=[]\n",
        "  targets=[]\n",
        "  All_X = []\n",
        "  all_non_interested_genes = []\n",
        "  new_sents = []\n",
        "  sentences = list(df[\"Sentence\"].unique())\n",
        "  IDs = []\n",
        "  ind = []\n",
        "  for sentence in sentences:\n",
        "    index = list(df[df['Sentence']==sentence][\"id\"])\n",
        "    IDs.append(index[0])\n",
        "  for x in range(len(sentences)):\n",
        "      X = query_raw(sentences[x])\n",
        "      #print(\"main\")\n",
        "      #print(sentences[x])\n",
        "      pos = return_gene_pos_pairs(sentences[x],X)\n",
        "      for j in range(len(pos)):\n",
        "          #print(\"------------------------------\")\n",
        "          #print(\"j here\",j)\n",
        "          #print(\"pos here\",len(pos))\n",
        "          genes = return_gene_pairs(sentences[x],X)\n",
        "          sent = sentences[x]\n",
        "          if(pos[j][0] != pos[j][1]):\n",
        "              agent = genes[j][0]\n",
        "              target = genes[j][1]\n",
        "              agent_placeholder = \"AGENTAGENTAGENTAGENTAGENTAGENTAGENTAGENTAGENTAGENT\"[0:len(agent)]\n",
        "              target_placeholder = \"TARGETTARGETTARGETTARGETTARGETTARGETTARGETTARGETTARGET\"[0:len(target)]\n",
        "              sent = sent[0:pos[j][0][0]]+agent_placeholder+sent[pos[j][0][1]:len(sent)]\n",
        "              sent = sent[0:pos[j][1][0]]+target_placeholder+sent[pos[j][1][1]:len(sent)]\n",
        "              sent = sent.replace(agent_placeholder,\"GENE1\").replace(target_placeholder,\"GENE2\")\n",
        "              #print(sentences[x])\n",
        "              #print(sent)\n",
        "              #print(\"--------------------\")\n",
        "          else:\n",
        "              #print(\"skipped\")\n",
        "              continue\n",
        "              #sent = sent[0:pos[j][0][0]]+\"SELFREGULATION\"+sent[pos[j][0][1]:len(sent)]\n",
        "              #print(sentences[x])\n",
        "              #print(sent)\n",
        "              #print(\"--------------------\")\n",
        "          ID.append(IDs[x])\n",
        "          All_X.append(X)\n",
        "          #print(\"j\",j)\n",
        "          agents.append(genes[j][0])\n",
        "          targets.append(genes[j][1])\n",
        "          sent_tagged.append(sent)\n",
        "          #print(\"step1\")\n",
        "          #print(sent)\n",
        "          sent_original.append(sentences[x])\n",
        "          ind.append(x)\n",
        "          genes = return_genes(sentences[x],X)\n",
        "          pos_2 = return_gene_pos(sentences[x],X)\n",
        "          all_non_interested_genes.append(genes)\n",
        "          cur_sent = sent\n",
        "          temp_sent = cur_sent\n",
        "          placeholders = []\n",
        "          for y in range(len(genes)):\n",
        "              if(\"GENE1\" not in genes[y] and \"GENE2\" not in genes[y]):\n",
        "                  #print('here')\n",
        "                  item = sent[pos_2[y][0]:pos_2[y][1]]\n",
        "                  length = len(item)\n",
        "                  placeholder = \"\"\n",
        "                  for l in range(length):\n",
        "                      placeholder = placeholder + str(y)\n",
        "                  if(len(placeholder)>0):\n",
        "                      placeholders.append(placeholder)\n",
        "\n",
        "                  temp_sent = temp_sent.replace(genes[y], placeholder)\n",
        "          #print(temp_sent)\n",
        "          temp_sent = temp_sent.replace(\"GENE1\",\"$GENE_AGENT#\").replace(\"GENE2\",\"$GENE_TARGET#\")\n",
        "          #print(placeholders)\n",
        "          for p in placeholders:\n",
        "              temp_sent = temp_sent.replace(p,\"BLANK\")\n",
        "          new_sents.append(temp_sent)\n",
        "          #print(\"------------------------------------\")\n",
        "\n",
        "          #print(x,\"x------------------------------------\")\n",
        "          #print(\"step2\")\n",
        "          #print(temp_sent)\n",
        "  new_df = pd.DataFrame()\n",
        "  new_df[\"ID\"]=ID\n",
        "  new_df['ind']=ind\n",
        "  new_df['Sent original'] = sent_original\n",
        "  new_df['Sent tagged'] = sent_tagged\n",
        "  new_df['Agent'] = agents\n",
        "  new_df['Target'] = targets\n",
        "  new_df[\"All_X\"] =All_X\n",
        "  new_df[\"all_non_interested_genes\"]=all_non_interested_genes\n",
        "  new_df[\"new_sents_blanks\"]=new_sents\n",
        "  new_df = new_df.fillna('')\n",
        "  return(new_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEIlPRtemKWX"
      },
      "source": [
        "# **Relation extraction- Relation classification:** The following cell contains the method for relation extraction. The model uses our fine-tunned pretrained BioBERT classifier relation_extraction_f.pth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-Rz3EkKJ7Yq"
      },
      "outputs": [],
      "source": [
        "#RE\n",
        "def relation_extraction(df):\n",
        "  model.load_state_dict(torch.load(F\"drive/MyDrive/E. Coli/relation_extraction_f.pth\" ))\n",
        "  model.cuda()\n",
        "  df_features = df[['new_sents_blanks']]\n",
        "  feature = df_features['new_sents_blanks']\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained('biobert_v1.1_pubmed', do_lower_case=True)\n",
        "\n",
        "  tokenized_texts = list(map(lambda t: ['[CLS]']+tokenizer.tokenize(t)+['[SEP]'] , feature))\n",
        "  #print (\"Tokenize the first sentence:\")\n",
        "  #print (tokenized_texts[1])\n",
        "  max_len = 256\n",
        "\n",
        "  input_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, tokenized_texts)),\n",
        "                              maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "  input_ids = pad_sequences(input_ids, maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  attention_masks = []\n",
        "\n",
        "  for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "  batch_size = 8\n",
        "  validation_inputs = torch.tensor(input_ids)\n",
        "  validation_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
        "\n",
        "  # Create an iterator of our data with torch DataLoader\n",
        "\n",
        "  validation_data = TensorDataset(validation_inputs, validation_masks)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "  all_predictions=[]\n",
        "  all_predictions_0 = []\n",
        "  all_predictions_1= []\n",
        "  all_predictions=[]\n",
        "  #true_labels = []\n",
        "  logits_abc = []\n",
        "  for batch in validation_dataloader:\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask = batch\n",
        "      # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "      with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        #label_ids = b_labels.to('cpu').numpy()\n",
        "        predicted_class = []\n",
        "        logits_abc.append(logits)\n",
        "        for x in range(len(logits)):\n",
        "          if(logits[x][0]>0):\n",
        "            abc=0\n",
        "          else:\n",
        "            abc=1\n",
        "          all_predictions.append(abc)\n",
        "        #tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        logits = logits.cpu()\n",
        "        logits = (tf.nn.sigmoid(logits).numpy())\n",
        "        #logits = logits.detach().cpu().numpy()\n",
        "        for x in range(len(logits)):\n",
        "          all_predictions_0.append(logits[x][0])\n",
        "          all_predictions_1.append(logits[x][1])\n",
        "          #all_predictions.append(abc)\n",
        "\n",
        "  df['prediction_RE']=all_predictions\n",
        "  df['prediction_RE_0']=all_predictions_0\n",
        "  df['prediction_RE_1']=all_predictions_1\n",
        "  df = df.fillna('')\n",
        "  return(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14308YeqnEry"
      },
      "source": [
        "# **Post-processing- Refinement:** The following cell contains the method for relation extraction. The model uses our fine-tunned pretrained BioBERT classifier relation_extraction_f.pth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y_6_VNp6kpg"
      },
      "outputs": [],
      "source": [
        "#remove Unwanted words\n",
        "def remove_unwanted_wds(df):\n",
        "  df_uw_2 = pd.read_excel(\"/content/unwanted_words.xlsx\")\n",
        "  agent_refined = []\n",
        "  for x in range(len(df)):\n",
        "      wrd = df['Agent'].iloc[x]\n",
        "      if(len(df['Agent'].iloc[x].split(\" \"))!=1):\n",
        "          wrd = \"\"\n",
        "          for y in df['Agent'].iloc[x].split(\" \"):\n",
        "              if(y not in list(df_uw_2['Unwanted_words'])):\n",
        "                wrd =wrd + y + \" \"\n",
        "      wrd = wrd.strip()\n",
        "      agent_refined.append(wrd)\n",
        "  target_refined = []\n",
        "  for x in range(len(df)):\n",
        "      wrd = df['Target'].iloc[x]\n",
        "      if(len(df['Target'].iloc[x].split(\" \"))!=1):\n",
        "          wrd = \"\"\n",
        "          for y in df['Target'].iloc[x].split(\" \"):\n",
        "              if(y not in list(df_uw_2['Unwanted_words'])):\n",
        "                wrd =wrd + y + \" \"\n",
        "      wrd = wrd.strip()\n",
        "      target_refined.append(wrd)\n",
        "  df['Agent_uws']=agent_refined\n",
        "  df['Target_uws']=target_refined\n",
        "  df = df.fillna('')\n",
        "  return(df)\n",
        "\n",
        "#remove non-gene words\n",
        "def remove_non_gene(df):\n",
        "  Refined_Agents = []\n",
        "  for y in range(len(df)):\n",
        "      #print('a',y)\n",
        "      temp = df['Agent_uws'].iloc[y]\n",
        "      #temp = temp.replace(\"-\",\" \")\n",
        "      temp = temp.split(\" \")\n",
        "      gene_name = ''\n",
        "      if(len(temp)>1):\n",
        "          for x in range(len(temp)):\n",
        "              if(len(temp[x])>0 and temp[x]!=\" \"):\n",
        "                  X = query_raw(temp[x])\n",
        "                  if('annotations' in list(X.keys())):\n",
        "                    if(len(X['annotations'])>0):\n",
        "                        if (X['annotations'][0]['obj'] == \"gene\"):\n",
        "                            gene_name = gene_name + temp[x] + \" \"\n",
        "          gene_name = gene_name.strip()\n",
        "          Refined_Agents.append(gene_name)\n",
        "      else:\n",
        "          Refined_Agents.append(temp[0])\n",
        "  Refined_Target = []\n",
        "  for y in range(len(df)):\n",
        "      #print('a',y)\n",
        "      temp = df['Target_uws'].iloc[y]\n",
        "      #temp = temp.replace(\"-\",\" \")\n",
        "      temp = temp.split(\" \")\n",
        "      gene_name = ''\n",
        "      if(len(temp)>1):\n",
        "          for x in range(len(temp)):\n",
        "              if(len(temp[x])>0 and temp[x]!=\" \"):\n",
        "                  X = query_raw(temp[x])\n",
        "                  if('annotations' in list(X.keys())):\n",
        "                    if(len(X['annotations'])>0):\n",
        "                        if (X['annotations'][0]['obj'] == \"gene\"):\n",
        "                            gene_name = gene_name + temp[x] + \" \"\n",
        "          gene_name = gene_name.strip()\n",
        "          Refined_Target.append(gene_name)\n",
        "      else:\n",
        "          Refined_Target.append(temp[0])\n",
        "  df['Agent_uws_gn']=Refined_Agents\n",
        "  df['Target_uws_gn']=Refined_Target\n",
        "  df = df.fillna('')\n",
        "  return(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27no6n8gonl8"
      },
      "source": [
        "# **Post-processing: Confidence computation:** The following cell contains the methods for computing confidence factor for extracted relations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW9llrK1Mkb5"
      },
      "outputs": [],
      "source": [
        "#Confidence\n",
        "\n",
        "def compute_confidence(df2):\n",
        "  confidence = []\n",
        "  unique_sentences = []\n",
        "  for pair in df2['Relation']:\n",
        "      score = 0\n",
        "      abc = df2[df2['Relation']==pair][['ID','Sent original','prediction_RE_1']]\n",
        "      unique_sentences.append(len(abc[\"Sent original\"].unique()))\n",
        "      for x in abc[\"Sent original\"].unique():\n",
        "          score = score + np.average(abc[abc['Sent original']==x]['prediction_RE_1'])\n",
        "      confidence.append(score)\n",
        "  df2['sentence_level_c'] = confidence\n",
        "  df2['re_unique_sentences']=unique_sentences #sentences giving the same relationship\n",
        "  return df2\n",
        "\n",
        "#following methods for only E Coli relations\n",
        "\n",
        "def if_e_coli_reg(df2):\n",
        "  df = pd.read_excel(\"/content/E_Coli_RegulonDB_regs.xlsx\")\n",
        "  rel_exists = []\n",
        "  for x in range(len(df2)):\n",
        "      agent = df2['Agent_uws_gn'].iloc[x]\n",
        "      target = df2['Target_uws_gn'].iloc[x]\n",
        "      rele = 0\n",
        "      for y in range(len(df)):\n",
        "          if(df['Agent'].iloc[y].lower() in agent.lower()):\n",
        "              if(df['Target'].iloc[y].lower() in target.lower()):\n",
        "                  rele = 1\n",
        "                  break\n",
        "\n",
        "      rel_exists.append(rele)\n",
        "  df2['E_Coli_regulation']=rel_exists\n",
        "  return df2\n",
        "\n",
        "def if_e_coli_regulon(df2):\n",
        "  df = pd.read_excel(\"/content/E_Coli_RegulonDB_regs.xlsx\")\n",
        "  E_Coli_regulons = []\n",
        "  for x in list(df['Agent'].unique()):\n",
        "      E_Coli_regulons.append(x.lower())\n",
        "  is_regulon = []\n",
        "  for agent in df2['Agent_uws_gn']:\n",
        "      is_r = 0\n",
        "      for ag in E_Coli_regulons:\n",
        "          if(ag.lower() in agent.lower().split(\" \")):\n",
        "              is_r=1\n",
        "\n",
        "              break\n",
        "      is_regulon.append(is_r)\n",
        "  df2['is_regulon']=is_regulon\n",
        "  return df2\n",
        "\n",
        "def final_conf(df2):\n",
        "  df2['c'] = df2['sentence_level_c'] + if_e_coli_regulon(df2)['is_regulon'] + 5*if_e_coli_reg(df2)['E_Coli_regulation']\n",
        "  return df2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_genes = pd.read_excel('/content/Ecoli_10_5.xlsx')\n",
        "df_final = pd.DataFrame()\n",
        "flag=0\n",
        "for x in df_genes['Genes']:\n",
        "  print(\"----------------------\")\n",
        "  #print(x)\n",
        "  Keywords = \"E Coli Escherichia coli gene regulation \"+x\n",
        "  #print(Keywords)\n",
        "  Article_count = 1000\n",
        "  max_len = 256\n",
        "  E_Coli = search(Keywords,Article_count)\n",
        "\n",
        "  E_Coli = search(Keywords,Article_count)\n",
        "\n",
        "  df_articles = extract_articles(E_Coli['IdList'])\n",
        "  df_articles = df_articles.dropna(axis=0)\n",
        "\n",
        "  df_sents_1 = tokenize_abs(df_articles)\n",
        "  if(len(df_sents_1)==0):\n",
        "\n",
        "      print(\"here1\")\n",
        "      continue\n",
        "  df_genes_1 = filter_sentences_genes(df_sents_1,list(df_genes['Genes']))\n",
        "  #print(df_genes_1)\n",
        "  df_genes_1 = df_genes_1.fillna('')\n",
        "  print(x)\n",
        "  print(df_genes_1['Gene'].value_counts())\n",
        "  for abc in list(df_genes_1['Gene'].value_counts().index[1:len(df_genes_1['Gene'].value_counts().index)]):\n",
        "    print(abc)\n",
        "    print(df_genes_1['Gene'].value_counts()[abc])\n",
        "    df333[x][df333['Genes']==abc]=df_genes_1['Gene'].value_counts()[abc]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qPJcbIoYphy",
        "outputId": "29fcc8c4-db83-45eb-ef31-74b215d2f4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n",
            "glnP\n",
            "        34\n",
            "glnP     7\n",
            "Name: Gene, dtype: int64\n",
            "glnP\n",
            "7\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-7afec3fd9b71>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df333[x][df333['Genes']==abc]=df_genes_1['Gene'].value_counts()[abc]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ihfB\n",
            "        52\n",
            "ihfB    17\n",
            "Name: Gene, dtype: int64\n",
            "ihfB\n",
            "17\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-7afec3fd9b71>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df333[x][df333['Genes']==abc]=df_genes_1['Gene'].value_counts()[abc]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here1\n",
            "----------------------\n",
            "pspG\n",
            "        94\n",
            "pspG     8\n",
            "Name: Gene, dtype: int64\n",
            "pspG\n",
            "8\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-7afec3fd9b71>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df333[x][df333['Genes']==abc]=df_genes_1['Gene'].value_counts()[abc]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rtcA\n",
            "    50\n",
            "Name: Gene, dtype: int64\n",
            "----------------------\n",
            "here1\n",
            "----------------------\n",
            "yiaJ\n",
            "        35\n",
            "yiaJ     3\n",
            "Name: Gene, dtype: int64\n",
            "yiaJ\n",
            "3\n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-7afec3fd9b71>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df333[x][df333['Genes']==abc]=df_genes_1['Gene'].value_counts()[abc]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yiaM\n",
            "    6981\n",
            "Name: Gene, dtype: int64\n",
            "----------------------\n",
            "here1\n",
            "----------------------\n",
            "yiaO\n",
            "yiaM    3\n",
            "        2\n",
            "Name: Gene, dtype: int64\n",
            "\n",
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-7afec3fd9b71>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df333[x][df333['Genes']==abc]=df_genes_1['Gene'].value_counts()[abc]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for abc in list(df_genes_1['Gene'].value_counts().index[1:len(df_genes_1['Gene'].value_counts().index)]):\n",
        "  print(abc)\n",
        "  print(df_genes_1['Gene'].value_counts()[abc])\n",
        "  df333[x][df333['Genes']==abc]=df_genes_1['Gene'].value_counts()[abc]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "yc7rniJLbNxe",
        "outputId": "db276acc-a7e2-4f0b-ddd6-cca3240ddc40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Row Labels'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-dc3f316133ee>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_genes_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Gene'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mdf333\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf333\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Row Labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_genes_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Gene'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Row Labels'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df333 = pd.read_excel(\"/content/Ecoli_10_5_p.xlsx\")\n",
        "#df333.to_excel(\"/content/Ecoli_10_5_pk.xlsx\")\n",
        "#pd.read_excel('/content/Ecoli_1_50.xlsx')\n",
        "df333"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "HdGfoM7Sb6bw",
        "outputId": "212907e7-f71e-4ec7-f052-25427fb406c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Genes  glnP  ihfB  lyxK  pspG  rtcA  sgbH  yiaJ  yiaM  yiaN  yiaO\n",
              "0  glnP     7     0     0     0     0     0     0     0     0     0\n",
              "1  ihfB     0    17     0     0     0     0     0     0     0     0\n",
              "2  lyxK     0     0     0     0     0     0     0     0     0     0\n",
              "3  pspG     0     0     0     8     0     0     0     0     0     0\n",
              "4  rtcA     0     0     0     0     0     0     0     0     0     0\n",
              "5  sgbH     0     0     0     0     0     0     0     0     0     0\n",
              "6  yiaJ     0     0     0     0     0     0     3     0     0     0\n",
              "7  yiaM     0     0     0     0     0     0     0     0     0     0\n",
              "8  yiaN     0     0     0     0     0     0     0     0     0     0\n",
              "9  yiaO     0     0     0     0     0     0     0     0     0     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4614fcc4-806d-4b4c-b18c-86695c6ad38b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Genes</th>\n",
              "      <th>glnP</th>\n",
              "      <th>ihfB</th>\n",
              "      <th>lyxK</th>\n",
              "      <th>pspG</th>\n",
              "      <th>rtcA</th>\n",
              "      <th>sgbH</th>\n",
              "      <th>yiaJ</th>\n",
              "      <th>yiaM</th>\n",
              "      <th>yiaN</th>\n",
              "      <th>yiaO</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>glnP</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ihfB</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lyxK</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>pspG</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>rtcA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>sgbH</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>yiaJ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>yiaM</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>yiaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>yiaO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4614fcc4-806d-4b4c-b18c-86695c6ad38b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4614fcc4-806d-4b4c-b18c-86695c6ad38b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4614fcc4-806d-4b4c-b18c-86695c6ad38b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwkbLPFhpSnh"
      },
      "source": [
        "**Step 7:** Initialize the selected keywords, number of abstracts you wish to fetch and token length for the classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "nuLdMMJA--T4",
        "outputId": "96c1fbc0-b322-4729-cf72-0fabf9612440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Genes'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e8462102aaa3>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"here1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mdf_genes_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_sentences_genes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_sents_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_genes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Genes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0;31m#print(df_genes_1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mdf_genes_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_genes_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Genes'"
          ]
        }
      ],
      "source": [
        "df_genes = pd.read_excel('/content/Ecoli1.xlsx')\n",
        "df_final = pd.DataFrame()\n",
        "flag=0\n",
        "for x in df_genes['gene']:\n",
        "  print(\"----------------------\")\n",
        "  #print(x)\n",
        "  Keywords = \"E Coli Escherichia coli gene regulation \"+x\n",
        "  #print(Keywords)\n",
        "  Article_count = 1000\n",
        "  max_len = 256\n",
        "  E_Coli = search(Keywords,Article_count)\n",
        "\n",
        "  E_Coli = search(Keywords,Article_count)\n",
        "\n",
        "  df_articles = extract_articles(E_Coli['IdList'])\n",
        "  df_articles = df_articles.dropna(axis=0)\n",
        "\n",
        "  df_sents_1 = tokenize_abs(df_articles)\n",
        "  if(len(df_sents_1)==0):\n",
        "\n",
        "      print(\"here1\")\n",
        "      continue\n",
        "  df_genes_1 = filter_sentences_genes(df_sents_1,list(df_genes['Genes']))\n",
        "  #print(df_genes_1)\n",
        "  df_genes_1 = df_genes_1.fillna('')\n",
        "  print(df_genes_1['Gene'].value_counts())\n",
        "  #df_sents_2 = sentence_classification_1(df_genes_1[df_genes_1['Contain']==1], max_len)\n",
        "  ##if(len(df_sents_2)==0):\n",
        "\n",
        "     #continue\n",
        "  #df_sents_2 = df_sents_2.fillna('')\n",
        "  #df_sents_1 = df_sents_1.fillna('')\n",
        "  #df_sents_1 = if_contains_genes(df_sents_1[df_sents_1['Contain']==1])\n",
        "  #df_sents_2 = sentence_classification_1(df_genes_1[df_genes_1['Contain']==1], max_len)\n",
        "  df_sents_NER = NER_1(df_genes_1[df_genes_1['Contain']==1])\n",
        "  if(len( df_sents_NER)==0):\n",
        "\n",
        "    print(\"here3\")\n",
        "    continue\n",
        "\n",
        "  df_sents_RE = relation_extraction(df_sents_NER)\n",
        "  df_RE_refined_1 = remove_unwanted_wds(df_sents_RE[df_sents_RE['prediction_RE']==1])\n",
        "  df_RE_refined_1 = df_RE_refined_1.fillna('')\n",
        "  if(len(df_RE_refined_1)==0):\n",
        "\n",
        "    print(\"here5\")\n",
        "    continue\n",
        "  df_RE_refined_2 = remove_non_gene(df_RE_refined_1)\n",
        "  df_RE_refined_2 = df_RE_refined_2.fillna('')\n",
        "  if(len(df_RE_refined_2)==0):\n",
        "\n",
        "    print(\"here4\")\n",
        "    continue\n",
        "  #df_RE_refined_2 = df_RE_refined_1\n",
        "  df_RE_refined_2 = df_RE_refined_2[df_RE_refined_2['Agent_uws_gn']!=\"\"]\n",
        "  df_RE_refined_2 = df_RE_refined_2[df_RE_refined_2['Target_uws_gn']!=\"\"]\n",
        "  df_RE_refined_2['Relation'] = df_RE_refined_2['Agent_uws_gn']+\"--\"+df_RE_refined_2['Target_uws_gn']\n",
        "\n",
        "  df_RE_scored = compute_confidence(df_RE_refined_2)\n",
        "  #for E Coli Regs\n",
        "  df_RE_scored_EColi = final_conf(df_RE_scored)\n",
        "  Final_relations = df_RE_scored_EColi[['Agent_uws_gn','Target_uws_gn','Relation','sentence_level_c','re_unique_sentences',\t'is_regulon',\t'E_Coli_regulation',\t'c']].drop_duplicates()\n",
        "  #--------------\n",
        "  print(\"Final Relations extracted as as follows\")\n",
        "  Final_relations.sort_values(['c'],ascending=False).to_excel(\"/content/drive/MyDrive/E. Coli/ExGI_result_10_5\"+str(x)+\".xlsx\")\n",
        "  Final_relations.sort_values(['c'],ascending=False)\n",
        "  if flag==0:\n",
        "    flag=1\n",
        "    df_final=Final_relations\n",
        "  else:\n",
        "    df_final = df_final.append(Final_relations, ignore_index=True)\n",
        "  print('Final_relations',len(Final_relations))\n",
        "  Final_relations.to_excel('drive/MyDrive/E. Coli/Ecoli_50_1_'+str(x)+'.xlsx')\n",
        "  df_final.to_excel('drive/MyDrive/E. Coli/Ecoli_50_1_df_final'+str(x)+'.xlsx')\n",
        "  print('df_final',len(df_final))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N7NaAcucDzPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_excel('/content/Ecoli1.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VToUS_wKDz3v",
        "outputId": "2bf64e9a-8086-481a-c40b-5ddd6b9b8e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    gene  cho  ddlB  dinB  dinD  dinF  dinG  dinI  dinJ  dinQ  ...  uvrC  \\\n",
              "0    cho    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "1   ddlB    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "2   dinB    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "3   dinD    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "4   dinF    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "5   dinG    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "6   dinI    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "7   dinJ    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "8   dinQ    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "9   dnaG    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "10  ftsI    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "11  ftsK    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "12  ftsL    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "13  ftsW    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "14  hokE    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "15  insK    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "16  lexA    1     1     1     1     1     1     1     1     1  ...     1   \n",
              "17  lpxC    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "18  mraY    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "19  murC    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "20  murD    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "21  murE    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "22  murF    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "23  murG    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "24   phr    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "25  polB    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "26  recA    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "27  recN    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "28  recX    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "29  rpoD    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "30  rpsU    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "31  ruvA    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "32  ruvB    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "33  sulA    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "34  symE    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "35  tisB    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "36  umuC    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "37  umuD    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "38  uvrA    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "39  uvrB    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "40  uvrC    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "41  uvrD    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "42  uvrY    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "43  yafN    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "44  yafO    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "45  yafP    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "46  yafQ    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "47  ybfE    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "48  ydjM    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "49  yebG    0     0     0     0     0     0     0     0     0  ...     0   \n",
              "\n",
              "    uvrD  uvrY  yafN  yafO  yafP  yafQ  ybfE  ydjM  yebG  \n",
              "0      0     0     0     0     0     0     0     0     0  \n",
              "1      0     0     0     0     0     0     0     0     0  \n",
              "2      0     0     0     0     0     0     0     0     0  \n",
              "3      0     0     0     0     0     0     0     0     0  \n",
              "4      0     0     0     0     0     0     0     0     0  \n",
              "5      0     0     0     0     0     0     0     0     0  \n",
              "6      0     0     0     0     0     0     0     0     0  \n",
              "7      0     0     0     0     0     0     0     0     0  \n",
              "8      0     0     0     0     0     0     0     0     0  \n",
              "9      0     0     0     0     0     0     0     0     0  \n",
              "10     0     0     0     0     0     0     0     0     0  \n",
              "11     0     0     0     0     0     0     0     0     0  \n",
              "12     0     0     0     0     0     0     0     0     0  \n",
              "13     0     0     0     0     0     0     0     0     0  \n",
              "14     0     0     0     0     0     0     0     0     0  \n",
              "15     0     0     0     0     0     0     0     0     0  \n",
              "16     1     1     1     1     1     1     1     1     1  \n",
              "17     0     0     0     0     0     0     0     0     0  \n",
              "18     0     0     0     0     0     0     0     0     0  \n",
              "19     0     0     0     0     0     0     0     0     0  \n",
              "20     0     0     0     0     0     0     0     0     0  \n",
              "21     0     0     0     0     0     0     0     0     0  \n",
              "22     0     0     0     0     0     0     0     0     0  \n",
              "23     0     0     0     0     0     0     0     0     0  \n",
              "24     0     0     0     0     0     0     0     0     0  \n",
              "25     0     0     0     0     0     0     0     0     0  \n",
              "26     0     0     0     0     0     0     0     0     0  \n",
              "27     0     0     0     0     0     0     0     0     0  \n",
              "28     0     0     0     0     0     0     0     0     0  \n",
              "29     0     0     0     0     0     0     0     0     0  \n",
              "30     0     0     0     0     0     0     0     0     0  \n",
              "31     0     0     0     0     0     0     0     0     0  \n",
              "32     0     0     0     0     0     0     0     0     0  \n",
              "33     0     0     0     0     0     0     0     0     0  \n",
              "34     0     0     0     0     0     0     0     0     0  \n",
              "35     0     0     0     0     0     0     0     0     0  \n",
              "36     0     0     0     0     0     0     0     0     0  \n",
              "37     0     0     0     0     0     0     0     0     0  \n",
              "38     0     0     0     0     0     0     0     0     0  \n",
              "39     0     0     0     0     0     0     0     0     0  \n",
              "40     0     0     0     0     0     0     0     0     0  \n",
              "41     0     0     0     0     0     0     0     0     0  \n",
              "42     0     0     0     0     0     0     0     0     0  \n",
              "43     0     0     0     0     0     0     0     0     0  \n",
              "44     0     0     0     0     0     0     0     0     0  \n",
              "45     0     0     0     0     0     0     0     0     0  \n",
              "46     0     0     0     0     0     0     0     0     0  \n",
              "47     0     0     0     0     0     0     0     0     0  \n",
              "48     0     0     0     0     0     0     0     0     0  \n",
              "49     0     0     0     0     0     0     0     0     0  \n",
              "\n",
              "[50 rows x 51 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-43b184bd-103f-4a90-b6cd-b5fa3f43b3bf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gene</th>\n",
              "      <th>cho</th>\n",
              "      <th>ddlB</th>\n",
              "      <th>dinB</th>\n",
              "      <th>dinD</th>\n",
              "      <th>dinF</th>\n",
              "      <th>dinG</th>\n",
              "      <th>dinI</th>\n",
              "      <th>dinJ</th>\n",
              "      <th>dinQ</th>\n",
              "      <th>...</th>\n",
              "      <th>uvrC</th>\n",
              "      <th>uvrD</th>\n",
              "      <th>uvrY</th>\n",
              "      <th>yafN</th>\n",
              "      <th>yafO</th>\n",
              "      <th>yafP</th>\n",
              "      <th>yafQ</th>\n",
              "      <th>ybfE</th>\n",
              "      <th>ydjM</th>\n",
              "      <th>yebG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cho</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ddlB</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dinB</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>dinD</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dinF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>dinG</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>dinI</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>dinJ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>dinQ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>dnaG</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ftsI</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ftsK</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>ftsL</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>ftsW</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>hokE</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>insK</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>lexA</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>lpxC</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>mraY</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>murC</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>murD</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>murE</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>murF</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>murG</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>phr</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>polB</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>recA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>recN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>recX</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>rpoD</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>rpsU</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>ruvA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>ruvB</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>sulA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>symE</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>tisB</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>umuC</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>umuD</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>uvrA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>uvrB</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>uvrC</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>uvrD</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>uvrY</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>yafN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>yafO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>yafP</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>yafQ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>ybfE</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>ydjM</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>yebG</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50 rows × 51 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43b184bd-103f-4a90-b6cd-b5fa3f43b3bf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-ca9de8ef-a98d-48d4-aa5d-376a6cd85322\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ca9de8ef-a98d-48d4-aa5d-376a6cd85322')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-ca9de8ef-a98d-48d4-aa5d-376a6cd85322 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-43b184bd-103f-4a90-b6cd-b5fa3f43b3bf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-43b184bd-103f-4a90-b6cd-b5fa3f43b3bf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h3Fd8M8DRwL"
      },
      "outputs": [],
      "source": [
        "Final_relations  = xxx\n",
        "ag_y = []\n",
        "tr_y = []\n",
        "for x in range(len(Final_relations)):\n",
        "    tempa = 0\n",
        "    for y in list(df_genes['Genes']):\n",
        "        if(y in Final_relations[\"Agent_uws_gn\"].iloc[x]):\n",
        "            tempa= 1\n",
        "            #ag_y.append(tempa)\n",
        "            continue\n",
        "    ag_y.append(tempa)\n",
        "    tempb = 0\n",
        "    for y in list(df_genes['Genes']):\n",
        "        if(y in Final_relations[\"Target_uws_gn\"].iloc[x]):\n",
        "            tempb= 1\n",
        "            #ag_y.append(tempa)\n",
        "            continue\n",
        "    tr_y.append(tempb)\n",
        "Final_relations['ag_y']=ag_y\n",
        "Final_relations['tr_y']=tr_y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_RE_refined_2 = df_RE_refined_2[df_RE_refined_2['Agent_uws_gn']!=\"\"]\n",
        "df_RE_refined_2 = df_RE_refined_2[df_RE_refined_2['Target_uws_gn']!=\"\"]\n",
        "df_RE_refined_2['Relation'] = df_RE_refined_2['Agent_uws_gn']+\"--\"+df_RE_refined_2['Target_uws_gn']\n",
        "\n",
        "df_RE_scored = compute_confidence(df_RE_refined_2)\n",
        "  #for E Coli Regs\n",
        "df_RE_scored_EColi = final_conf(df_RE_scored)\n",
        "Final_relations = df_RE_scored_EColi[['Agent_uws_gn','Target_uws_gn','Relation','sentence_level_c','re_unique_sentences',\t'is_regulon',\t'E_Coli_regulation',\t'c']].drop_duplicates()\n",
        "  #--------------\n",
        "print(\"Final Relations extracted as as follows\")\n",
        "Final_relations.sort_values(['c'],ascending=False).to_excel(\"/content/drive/MyDrive/E. Coli/ExGI_result\"+str(x)+\".xlsx\")\n",
        "Final_relations.sort_values(['c'],ascending=False)\n",
        "if flag==0:\n",
        "  flag=1\n",
        "  df_final=Final_relations\n",
        "else:\n",
        "  df_final = df_final.append(Final_relations, ignore_index=True)\n",
        "  print('Final_relations',len(Final_relations))\n",
        "  Final_relations.to_excel('drive/MyDrive/E. Coli/Ecoli_3_'+str(x)+'.xlsx')\n",
        "  df_final.to_excel('drive/MyDrive/E. Coli/Ecoli_3_df_final'+str(x)+'.xlsx')\n",
        "  print('df_final',len(df_final))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSBVRIt4A_Bx",
        "outputId": "a4d86f04-c7d3-474d-8495-96361a46c06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Relations extracted as as follows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Final_relations.to_excel(\"xxx.xlsx\")"
      ],
      "metadata": {
        "id": "_LGHRjq6M0We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eySPWTT-E_dh"
      },
      "outputs": [],
      "source": [
        "df_articles.to_excel(\"drive/MyDrive/E. Coli/e_coli_2_articles.xlsx\")\n",
        "df_final.to_excel(\"drive/MyDrive/E. Coli/e_coli_2_rels.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-ITgFxCE3WS"
      },
      "outputs": [],
      "source": [
        "Keywords = \"E Coli Escherichia coli gene regulation gene expression transcriptional\"\n",
        "Article_count = 1000\n",
        "max_len = 256\n",
        "Keywords = \"gene regulation transcriptional Bacillus subtilis\"\n",
        "E_Coli = search(Keywords,Article_count)\n",
        "#df_genes = pd.read_excel('/content/SOS_DNA_9Genes_Network.xlsx')\n",
        "#E_Coli = search(Keywords,Article_count)\n",
        "df_articles = extract_articles(E_Coli['IdList'])\n",
        "df_articles = df_articles.dropna(axis=0)\n",
        "df_sents_1 = tokenize_abs(df_articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfYD2JV_tVm1"
      },
      "outputs": [],
      "source": [
        "#df_genes_1['Gene'].value_counts()\n",
        "#np.sum(df_genes_1['Contain'])\n",
        "#len(df_articles)\n",
        "df_genes = pd.read_excel('/content/SOS_DNA_9Genes_Network.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "U4LoFN-RNADX",
        "outputId": "46d9df7a-5320-4a02-b5bb-775ab5ceca1e"
      },
      "outputs": [
        {
          "ename": "UFuncTypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9be105acc298>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_RE_refined_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Agent_uws_gn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"--\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdf_RE_refined_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Target_uws_gn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__add__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__radd__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign_method_SERIES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexOpsMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_cmp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('float64'), dtype('<U2')) -> None"
          ]
        }
      ],
      "source": [
        "df_RE_refined_2['Relation'] = df_RE_refined_2['Agent_uws_gn']+\"--\"+df_RE_refined_2['Target_uws_gn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8_-aHVApgWB"
      },
      "source": [
        "**Step 8:** Run the following cell to extract the relations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "v1DwOKd5_BJq",
        "outputId": "08d1dc80-b6ff-4b81-e6ee-3532326f25fa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-fdc46f042a96>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_genes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Ecoli_1.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mE_Coli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeywords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mArticle_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE_Coli\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IdList'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_articles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1650\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1653\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1526\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Ecoli_1.xlsx'"
          ]
        }
      ],
      "source": [
        "df_genes = pd.read_excel('/content/Ecoli_1.xlsx')\n",
        "E_Coli = search(Keywords,Article_count)\n",
        "\n",
        "df_articles = extract_articles(E_Coli['IdList'])\n",
        "df_articles = df_articles.dropna(axis=0)\n",
        "df_sents_1 = tokenize_abs(df_articles)\n",
        "\n",
        "df_genes_1 = filter_sentences_genes(df_sents_all,list(df_genes['Genes']))\n",
        "\n",
        "print(df_genes_1['Gene'].value_counts())\n",
        "df_sents_2 = sentence_classification_1(df_genes_1[df_genes_1['Contain']==1], max_len)\n",
        "df_genes_1 = df_genes_1.fillna('')\n",
        "b = 6\n",
        "abc = int(len(df_genes_1[df_genes_1['Contain']==1])/b)\n",
        "\n",
        "df_sents_3 = pd.DataFrame()\n",
        "for x in range(b):\n",
        "  df_temp = df_genes_1[df_genes_1['Contain']==1][x*abc: (x+1)*abc]\n",
        "  #print(x,abc)\n",
        "  df_sents_3_t = if_contains_genes(df_genes_1[df_genes_1['Contain']==1])\n",
        "  if(x == 0 ):\n",
        "    df_sents_3 = df_sents_3_t\n",
        "  else:\n",
        "    df_sents_3 = df_sents_3.append(df_sents_3_t, ignore_index=True)\n",
        "df_sents_3 = df_sents_3.fillna('')\n",
        "df_sents_NER = NER_1(df_sents_3[df_sents_3[\"gene_mention\"]==1])\n",
        "df_sents_RE = relation_extraction(df_sents_NER)\n",
        "df_RE_refined_1 = remove_unwanted_wds(df_sents_RE[df_sents_RE['prediction_RE']==1])\n",
        "df_RE_refined_1 = df_RE_refined_1.fillna('')\n",
        "df_RE_refined_2 = remove_non_gene(df_RE_refined_1)\n",
        "df_RE_refined_2 = df_RE_refined_2.fillna('')\n",
        "df_RE_refined_2 = df_RE_refined_2[df_RE_refined_2['Agent_uws_gn']!=\"\"]\n",
        "df_RE_refined_2 = df_RE_refined_2[df_RE_refined_2['Target_uws_gn']!=\"\"]\n",
        "df_RE_refined_2['Relation'] = df_RE_refined_2['Agent_uws_gn']+\"--\"+df_RE_refined_2['Target_uws_gn']\n",
        "df_RE_scored = compute_confidence(df_RE_refined_2)\n",
        "#for E Coli Regs\n",
        "df_RE_scored_EColi = final_conf(df_RE_scored)\n",
        "Final_relations = df_RE_scored_EColi[['Agent_uws_gn','Target_uws_gn','Relation','sentence_level_c','re_unique_sentences',\t'is_regulon',\t'E_Coli_regulation',\t'c']].drop_duplicates()\n",
        "#--------------\n",
        "print(\"Final Relations extracted as as follows\")\n",
        "print(Final_relations)\n",
        "Final_relations.sort_values(['c'],ascending=False).to_excel(\"ExGI_result.xlsx\")\n",
        "Final_relations.sort_values(['c'],ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUAEwjW1T8Rm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "i6mSOHMM_MxL",
        "outputId": "8b2c39e7-01b4-4110-b8e4-f99858f012b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Relations extracted as as follows\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e09264f2-7cf3-46de-8441-48772c5c35c7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Agent_uws_gn</th>\n",
              "      <th>Target_uws_gn</th>\n",
              "      <th>Relation</th>\n",
              "      <th>sentence_level_c</th>\n",
              "      <th>re_unique_sentences</th>\n",
              "      <th>is_regulon</th>\n",
              "      <th>E_Coli_regulation</th>\n",
              "      <th>c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>LexA</td>\n",
              "      <td>SOS</td>\n",
              "      <td>LexA--SOS</td>\n",
              "      <td>14.298716</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>15.298716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>SOS</td>\n",
              "      <td>LexA</td>\n",
              "      <td>SOS--LexA</td>\n",
              "      <td>13.787022</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13.787022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>LexA</td>\n",
              "      <td>RecA</td>\n",
              "      <td>LexA--RecA</td>\n",
              "      <td>6.913495</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12.913495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>LexA</td>\n",
              "      <td>LexA</td>\n",
              "      <td>LexA--LexA</td>\n",
              "      <td>5.898354</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>11.898354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1946</th>\n",
              "      <td>lexA</td>\n",
              "      <td>recA</td>\n",
              "      <td>lexA--recA</td>\n",
              "      <td>5.794406</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>11.794406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2086</th>\n",
              "      <td>recA</td>\n",
              "      <td>recA430</td>\n",
              "      <td>recA--recA430</td>\n",
              "      <td>0.551123</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.551123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1409</th>\n",
              "      <td>SOS</td>\n",
              "      <td>ydjM</td>\n",
              "      <td>SOS--ydjM</td>\n",
              "      <td>0.537277</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.537277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404</th>\n",
              "      <td>recA(E38K)</td>\n",
              "      <td>SOS</td>\n",
              "      <td>recA(E38K)--SOS</td>\n",
              "      <td>0.521291</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.521291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>colicin</td>\n",
              "      <td>lexA</td>\n",
              "      <td>colicin--lexA</td>\n",
              "      <td>0.499577</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.499577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1145</th>\n",
              "      <td>UmuDAb</td>\n",
              "      <td>LexA</td>\n",
              "      <td>UmuDAb--LexA</td>\n",
              "      <td>0.496232</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.496232</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>336 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e09264f2-7cf3-46de-8441-48772c5c35c7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e09264f2-7cf3-46de-8441-48772c5c35c7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e09264f2-7cf3-46de-8441-48772c5c35c7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Agent_uws_gn Target_uws_gn         Relation  sentence_level_c  \\\n",
              "58           LexA           SOS        LexA--SOS         14.298716   \n",
              "63            SOS          LexA        SOS--LexA         13.787022   \n",
              "89           LexA          RecA       LexA--RecA          6.913495   \n",
              "148          LexA          LexA       LexA--LexA          5.898354   \n",
              "1946         lexA          recA       lexA--recA          5.794406   \n",
              "...           ...           ...              ...               ...   \n",
              "2086         recA       recA430    recA--recA430          0.551123   \n",
              "1409          SOS          ydjM        SOS--ydjM          0.537277   \n",
              "404    recA(E38K)           SOS  recA(E38K)--SOS          0.521291   \n",
              "411       colicin          lexA    colicin--lexA          0.499577   \n",
              "1145       UmuDAb          LexA     UmuDAb--LexA          0.496232   \n",
              "\n",
              "      re_unique_sentences  is_regulon  E_Coli_regulation          c  \n",
              "58                     15           1                  0  15.298716  \n",
              "63                     14           0                  0  13.787022  \n",
              "89                      7           1                  1  12.913495  \n",
              "148                     6           1                  1  11.898354  \n",
              "1946                    6           1                  1  11.794406  \n",
              "...                   ...         ...                ...        ...  \n",
              "2086                    1           0                  0   0.551123  \n",
              "1409                    1           0                  0   0.537277  \n",
              "404                     1           0                  0   0.521291  \n",
              "411                     1           0                  0   0.499577  \n",
              "1145                    1           0                  0   0.496232  \n",
              "\n",
              "[336 rows x 8 columns]"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#df_sents_3 = df_sents_3.fillna('')\n",
        "#df_sents_NER_1 = NER_1(df_sents_3[df_sents_3[\"gene_mention\"]==1].iloc[0:18])\n",
        "#df_sents_NER_2 = NER_1(df_sents_3[df_sents_3[\"gene_mention\"]==1])\n",
        "#df_sents_RE = relation_extraction(df_sents_NER_2)\n",
        "#df_RE_refined_1 = remove_unwanted_wds(df_sents_RE[df_sents_RE['prediction_RE']==1])\n",
        "#df_RE_refined_1 = df_RE_refined_1.fillna('')\n",
        "df_RE_refined_2 = remove_non_gene(df_RE_refined_1)\n",
        "df_RE_refined_2 = df_RE_refined_2.fillna('')\n",
        "df_RE_refined_2 = df_RE_refined_2[df_RE_refined_2['Agent_uws_gn']!=\"\"]\n",
        "df_RE_refined_2 = df_RE_refined_2[df_RE_refined_2['Target_uws_gn']!=\"\"]\n",
        "df_RE_refined_2['Relation'] = df_RE_refined_2['Agent_uws_gn']+\"--\"+df_RE_refined_2['Target_uws_gn']\n",
        "df_RE_scored = compute_confidence(df_RE_refined_2)\n",
        "#for E Coli Regs\n",
        "df_RE_scored_EColi = final_conf(df_RE_scored)\n",
        "Final_relations = df_RE_scored_EColi[['Agent_uws_gn','Target_uws_gn','Relation','sentence_level_c','re_unique_sentences',\t'is_regulon',\t'E_Coli_regulation',\t'c']].drop_duplicates()\n",
        "#--------------\n",
        "print(\"Final Relations extracted as as follows\")\n",
        "Final_relations.sort_values(['c'],ascending=False).to_excel(\"ExGI_result.xlsx\")\n",
        "Final_relations.sort_values(['c'],ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZLgHI05Y84X"
      },
      "outputs": [],
      "source": [
        "Final_relations = df_final\n",
        "ag_y = []\n",
        "tr_y = []\n",
        "for x in range(len(Final_relations)):\n",
        "  if Final_relations[\"Agent_uws_gn\"].iloc[x] in list(df_genes['Genes']):\n",
        "    ag_y.append(1)\n",
        "  else:\n",
        "    ag_y.append(0)\n",
        "  if Final_relations[\"Target_uws_gn\"].iloc[x] in list(df_genes['Genes']):\n",
        "    tr_y.append(1)\n",
        "  else:\n",
        "    tr_y.append(0)\n",
        "Final_relations['ag_y']=ag_y\n",
        "Final_relations['tr_y']=tr_y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Final_relations.to_excel(\"E_Coli_1_final_rels.xlsx\")"
      ],
      "metadata": {
        "id": "tPrB0jkKRkWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdKnRobEtI5k",
        "outputId": "f0e403c6-1060-46db-966c-b654667fce1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 625\n",
            "Sentence Eliminator 2: eliminated 182 out of  625\n",
            "4 625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-29-0c67cf66a394>:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_sents_3 = df_sents_3.append(df_sents_3_t, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence Eliminator 2: eliminated 150 out of  625\n",
            "5 625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-29-0c67cf66a394>:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_sents_3 = df_sents_3.append(df_sents_3_t, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence Eliminator 2: eliminated 115 out of  625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-29-0c67cf66a394>:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_sents_3 = df_sents_3.append(df_sents_3_t, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "for x in range(3,b):\n",
        "  df_temp = df_genes_1[df_genes_1['Contain']==1][x*abc: (x+1)*abc]\n",
        "  print(x,abc)\n",
        "  df_sents_3_t = if_contains_genes(df_temp)\n",
        "  if(x == 0 ):\n",
        "    df_sents_3 = df_sents_3_t\n",
        "  else:\n",
        "    df_sents_3 = df_sents_3.append(df_sents_3_t, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ-FB9h7_XwJ",
        "outputId": "efde9ba6-d65a-4586-de03-21506fffad11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_sents_3)/625"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo5iqKkYZvlj"
      },
      "outputs": [],
      "source": [
        "Final_relations.sort_values(['c'],ascending=False).to_excel(\"ExGI_result.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3JhB8KP_z0y"
      },
      "outputs": [],
      "source": [
        "def NER_1(df):\n",
        "  #print(len(df))\n",
        "  ID = []\n",
        "  sent_tagged = []\n",
        "  sent_original = []\n",
        "  pair = []\n",
        "  add_lengths = 0\n",
        "  agents=[]\n",
        "  targets=[]\n",
        "  All_X = []\n",
        "  all_non_interested_genes = []\n",
        "  new_sents = []\n",
        "  sentences = list(df[\"Sentence\"].unique())\n",
        "  IDs = []\n",
        "  ind = []\n",
        "  for sentence in sentences:\n",
        "    index = list(df[df['Sentence']==sentence][\"id\"])\n",
        "    IDs.append(index[0])\n",
        "  for x in range(len(sentences)):\n",
        "      X = query_raw(sentences[x])\n",
        "      #print(sentences[x])\n",
        "      #print(\"main\")\n",
        "      #print(sentences[x])\n",
        "      pos = return_gene_pos_pairs(sentences[x],X)\n",
        "      for j in range(len(pos)):\n",
        "          #print(\"------------------------------\")\n",
        "          #print(\"j here\",j)\n",
        "          #print(\"pos here\",len(pos))\n",
        "          genes = return_gene_pairs(sentences[x],X)\n",
        "          sent = sentences[x]\n",
        "          if(pos[j][0] != pos[j][1]):\n",
        "              agent = genes[j][0]\n",
        "              target = genes[j][1]\n",
        "              agent_placeholder = \"*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!\"[0:len(agent)]\n",
        "              target_placeholder = \"^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#\"[0:len(target)]\n",
        "              #print(sent)\n",
        "              #print(pos[j])\n",
        "              sent = sent[0:pos[j][0][0]]+agent_placeholder+sent[pos[j][0][1]:len(sent)]\n",
        "              #print(sent)\n",
        "              #print(pos[j])\n",
        "              sent = sent[0:pos[j][1][0]]+target_placeholder+sent[pos[j][1][1]:len(sent)]\n",
        "              #print(sent)\n",
        "              #print(pos[j])\n",
        "              sent = sent.replace(agent_placeholder,\"GENE1\").replace(target_placeholder,\"GENE2\")\n",
        "              #print(sentences[x])\n",
        "              #print(sent)\n",
        "              #print(\"--------------------\")\n",
        "          else:\n",
        "              #print(\"skipped\")\n",
        "              continue\n",
        "              #sent = sent[0:pos[j][0][0]]+\"SELFREGULATION\"+sent[pos[j][0][1]:len(sent)]\n",
        "              #print(sentences[x])\n",
        "              #print(sent)\n",
        "              #print(\"--------------------\")\n",
        "          ID.append(IDs[x])\n",
        "          All_X.append(X)\n",
        "          #print(\"j\",j)\n",
        "          agents.append(genes[j][0])\n",
        "          targets.append(genes[j][1])\n",
        "          sent_tagged.append(sent)\n",
        "          #print(\"step1\")\n",
        "          #print(sent)\n",
        "          sent_original.append(sentences[x])\n",
        "          ind.append(x)\n",
        "          genes = return_genes(sentences[x],X)\n",
        "          pos_2 = return_gene_pos(sentences[x],X)\n",
        "          all_non_interested_genes.append(genes)\n",
        "          cur_sent = sent\n",
        "          temp_sent = cur_sent\n",
        "          placeholders = []\n",
        "\n",
        "          for y in range(len(genes)):\n",
        "              #print(genes)\n",
        "              if(\"GENE1\" not in genes[y] and \"GENE2\" not in genes[y]):\n",
        "                  #print('here')\n",
        "                  #print(genes[y])\n",
        "                  #print(sent)\n",
        "                  #print(pos_2)\n",
        "                  try:\n",
        "                    item = sent[pos_2[y][0]:pos_2[y][1]]\n",
        "                    length = len(item)\n",
        "                    placeholder = \"\"\n",
        "                    for l in range(length):\n",
        "                        placeholder = placeholder + str(y)\n",
        "                    if(len(placeholder)>0):\n",
        "                        placeholders.append(placeholder)\n",
        "                  except:\n",
        "                    continue\n",
        "\n",
        "\n",
        "                  temp_sent = temp_sent.replace(genes[y], placeholder)\n",
        "          #print(temp_sent)\n",
        "          temp_sent = temp_sent.replace(\"GENE1\",\"$GENE_AGENT#\").replace(\"GENE2\",\"$GENE_TARGET#\")\n",
        "          #print(placeholders)\n",
        "          for p in placeholders:\n",
        "              temp_sent = temp_sent.replace(p,\"BLANK\")\n",
        "          new_sents.append(temp_sent)\n",
        "          #print(\"------------------------------------\")\n",
        "\n",
        "          #print(x,\"x------------------------------------\")\n",
        "          #print(\"step2\")\n",
        "          #print(temp_sent)\n",
        "  new_df = pd.DataFrame()\n",
        "  new_df[\"ID\"]=ID\n",
        "  new_df['ind']=ind\n",
        "  new_df['Sent original'] = sent_original\n",
        "  new_df['Sent tagged'] = sent_tagged\n",
        "  new_df['Agent'] = agents\n",
        "  new_df['Target'] = targets\n",
        "  new_df[\"All_X\"] =All_X\n",
        "  new_df[\"all_non_interested_genes\"]=all_non_interested_genes\n",
        "  new_df[\"new_sents_blanks\"]=new_sents\n",
        "  new_df = new_df.fillna('')\n",
        "  return(new_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "Vl0yIHzPagEV",
        "outputId": "20f678c3-ac2d-48fd-f0ae-8974ecff3c94"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-a44c8767275b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_sents_NER_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_sents_NER_2' is not defined"
          ]
        }
      ],
      "source": [
        "df_sents_NER_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcevPX3DZvvV"
      },
      "outputs": [],
      "source": [
        "df_genes_1 = filter_sentences_genes(df_sents_1,list(df_genes['Genes']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTSG5grGaBQ4",
        "outputId": "ffb65f10-50e8-4229-cfcb-76c67caef8a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "362"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sum(filter_sentences_genes(df_genes_1,list(df_genes['Genes']))['Contain'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG8EXKpacZ6T",
        "outputId": "b7147782-b04b-4bf3-d26d-8c4c2dea8d6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "        1080\n",
              "LexA     139\n",
              "recA      96\n",
              "lexA      70\n",
              "uvrA      25\n",
              "umuD      23\n",
              "uvrD       6\n",
              "polB       2\n",
              "ruvA       1\n",
              "Name: Gene, dtype: int64"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_genes_1['Gene'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uUPuoUgtDu5X",
        "outputId": "599b5ec3-89cf-40a6-a81d-6c9046e3ec93"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'LexA recA lexA umuD uvrD uvrA uvrY ruvA polB'"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\" \".join(list(df_genes['Genes']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "oFAXppfkFX_7",
        "outputId": "8f9344e0-082f-4545-b848-07f39328ec07"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1603c4de-838e-4148-a364-0c39ad0536d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Contain</th>\n",
              "      <th>Gene</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>3</td>\n",
              "      <td>The bacterial DNA damage response pathway (SOS...</td>\n",
              "      <td>1</td>\n",
              "      <td>LexA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>3</td>\n",
              "      <td>The lexA promoter, itself, contains two LexA o...</td>\n",
              "      <td>1</td>\n",
              "      <td>lexA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>3</td>\n",
              "      <td>To test this, we quantified 5mC at the lexA lo...</td>\n",
              "      <td>1</td>\n",
              "      <td>lexA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>3</td>\n",
              "      <td>We found that 5mC at the lexA promoter is spec...</td>\n",
              "      <td>1</td>\n",
              "      <td>lexA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>3</td>\n",
              "      <td>Our data support a model where LexA binding at...</td>\n",
              "      <td>1</td>\n",
              "      <td>LexA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1422</th>\n",
              "      <td>171</td>\n",
              "      <td>These data can be explained by a model in whic...</td>\n",
              "      <td>1</td>\n",
              "      <td>LexA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1424</th>\n",
              "      <td>172</td>\n",
              "      <td>A comparison of these sequences with those of ...</td>\n",
              "      <td>1</td>\n",
              "      <td>lexA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1425</th>\n",
              "      <td>172</td>\n",
              "      <td>Using both, a thermosensitive lexA host strain...</td>\n",
              "      <td>1</td>\n",
              "      <td>lexA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1426</th>\n",
              "      <td>172</td>\n",
              "      <td>From the data presented, we conclude that bact...</td>\n",
              "      <td>1</td>\n",
              "      <td>lexA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1430</th>\n",
              "      <td>173</td>\n",
              "      <td>This is expressed in increased UV-sensitivity ...</td>\n",
              "      <td>1</td>\n",
              "      <td>recA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>362 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1603c4de-838e-4148-a364-0c39ad0536d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1603c4de-838e-4148-a364-0c39ad0536d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1603c4de-838e-4148-a364-0c39ad0536d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       id                                           Sentence  Contain  Gene\n",
              "28      3  The bacterial DNA damage response pathway (SOS...        1  LexA\n",
              "29      3  The lexA promoter, itself, contains two LexA o...        1  lexA\n",
              "31      3  To test this, we quantified 5mC at the lexA lo...        1  lexA\n",
              "32      3  We found that 5mC at the lexA promoter is spec...        1  lexA\n",
              "33      3  Our data support a model where LexA binding at...        1  LexA\n",
              "...   ...                                                ...      ...   ...\n",
              "1422  171  These data can be explained by a model in whic...        1  LexA\n",
              "1424  172  A comparison of these sequences with those of ...        1  lexA\n",
              "1425  172  Using both, a thermosensitive lexA host strain...        1  lexA\n",
              "1426  172  From the data presented, we conclude that bact...        1  lexA\n",
              "1430  173  This is expressed in increased UV-sensitivity ...        1  recA\n",
              "\n",
              "[362 rows x 4 columns]"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_sents_1[df_sents_1['Contain']==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqAiHlErFyw2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "T4"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}