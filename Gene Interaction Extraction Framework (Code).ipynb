{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvHTKTtPiEPA"
      },
      "source": [
        "# ExGI - Gene Interaction Extraction algorithm\n",
        "\n",
        "\n",
        "Welcome to ExGI, an algorithm designed for extracting gene interactions from scientific articles. This algorithm employs a combination of natural language processing (NLP) techniques, named entity recognition (NER), and relation extraction to identify and refine gene interactions within the provided text.\n",
        "\n",
        "## Overview:\n",
        "The ExGI algorithm involves several key steps:\n",
        "\n",
        "1. **Pre-processing:** Extract and preprocess article data, including abstract tokenization and sentence elimination.\n",
        "2. **Relation Extraction:** Utilise a fine-tuned bioBERT model to extract gene interactions.\n",
        "5. **Post-processing:** Refine entity names and Compute confidence scores for extracted relations, considering multiple factors.\n",
        "\n",
        "## Important Note:\n",
        "Please be aware that Named Entity Recoginition functionalities of this code rely on a RESTful API provided by http://bern2.korea.ac.kr/. It's important to note that the API may experience intermittent unresponsiveness or delays, especially with multiple concurrent requests. If issues persist, refer to BERN2 documentation(http://bern2.korea.ac.kr/documentation) for guidance on implementing a local installation.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following two cell imports the needed packages, pretrained BioBERT weights and loads pretrained BioBert model\n"
      ],
      "metadata": {
        "id": "qGker2ZA7wR8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxP3wpXGNCbe"
      },
      "outputs": [],
      "source": [
        "# Extract BioBERT weights\n",
        "!wget http://nlp.dmis.korea.edu/projects/biobert-2020-checkpoints/biobert_v1.1_pubmed.tar.gz\n",
        "!tar -xvzf /content/biobert_v1.1_pubmed.tar.gz\n",
        "\n",
        "# Install required packages\n",
        "!pip install pytorch_transformers\n",
        "!pip install transformers\n",
        "!tar -xzf biobert_weights\n",
        "!ls biobert_v1.1_pubmed/\n",
        "!transformers-cli convert --model_type bert --tf_checkpoint biobert_v1.1_pubmed/model.ckpt-1000000 --config biobert_v1.1_pubmed/bert_config.json --pytorch_dump_output biobert_v1.1_pubmed/pytorch_model.bin\n",
        "!ls biobert_v1.1_pubmed/\n",
        "!mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json\n",
        "!ls biobert_v1.1_pubmed/\n",
        "!pip install datasets\n",
        "from pytorch_transformers import BertModel\n",
        "DATA_DIR=\".\"\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "# Installing additional packages\n",
        "!pip install biopython\n",
        "!pip install Bio\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# install BERT\n",
        "!pip install pytorch_pretrained_bert pytorch-nlp\n",
        "\n",
        "# BERT imports\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "!pip install Keras-Preprocessing\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n",
        "model = BertForSequenceClassification.from_pretrained(\"biobert_v1.1_pubmed\", num_labels=2)#binary classification\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWIEdpHiiBOC"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bs4\n",
        "from urllib.request import urlopen as uReq\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from openpyxl import load_workbook\n",
        "from Bio import Entrez\n",
        "import nltk.data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import requests\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Variable Initialisation\n",
        "\n",
        "#Initializing email ID for PubMed Entrez search query fetch\n",
        "mail = 'jyour_email@example.com' # Replace with your actual email ID\n",
        "\n",
        "# Ensure to provide the correct path where you have saved the 'sentence_extraction_f.pth' file\n",
        "sentence_classifier = 'sentence_extraction_f.pth'\n",
        "\n",
        "# Ensure to provide the correct path where you have saved the 'relation_extraction_f.pth' file\n",
        "relation_extraction_model = F\"relation_extraction_f.pth\"\n",
        "\n",
        "# Ensure to provide the correct path where you have saved the 'unwanted_words.xlsx' file\n",
        "unwanted_words_list = 'unwanted_words.xlsx'\n",
        "\n",
        "# Ensure to provide the correct path where you have saved the 'E_Coli_RegulonDB_regs.xlsx' file\n",
        "E_Coli_Regulations  = \"E_Coli_RegulonDB_regs.xlsx\"\n",
        "\n",
        "# Keywords for PubMed Entrez search query to extract regulatory relationships for E. Coli, based on the paper\n",
        "# Please refer to the paper for the rationale behind selecting these keywords\n",
        "Keywords = \"E Coli Escherichia coli gene regulation gene expression transcriptional\"\n",
        "\n",
        "# Number of articles to fetch from PubMed\n",
        "Article_count = 100\n",
        "\n",
        "# Maximum length of tokens for data processing related to BioBERT models\n",
        "max_len = 256"
      ],
      "metadata": {
        "id": "H8ubVvLdVhyU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oGvtDTzi9ac"
      },
      "source": [
        "# **Pre-processing - Search query with target specific keywords and retrieve relevant abstracts:** The following cell contains the methods to fetch literature abstracts as per selected keywords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EAodqCsSLUAj"
      },
      "outputs": [],
      "source": [
        "# Function to search for articles on PubMed based on a given query and retrieve a specified number of results\n",
        "\n",
        "def search(query,amt):\n",
        "    # Set the email for PubMed API access\n",
        "    Entrez.email = mail\n",
        "\n",
        "    # Perform the PubMed search and retrieve results\n",
        "    handle = Entrez.esearch(db='pubmed',\n",
        "                            sort='relevance',\n",
        "                            retmax=amt,\n",
        "                            retmode='xml',\n",
        "                            term=query)\n",
        "    results = Entrez.read(handle)\n",
        "    return results\n",
        "\n",
        "# Function to generate a PubMed link for a given article ID\n",
        "def make_a_link(id):\n",
        "    return(\"https://pubmed.ncbi.nlm.nih.gov/\"+str(id)+\"/\")\n",
        "\n",
        "# Function to extract relevant information from a list of PubMed article IDs\n",
        "def extract_articles(list_of_ids):\n",
        "\n",
        "    # Lists to store extracted information\n",
        "    Article_title = []\n",
        "    Article_Abstract = []\n",
        "    Pub_med_link = []\n",
        "    Link_full_articles = []\n",
        "    Type_of_articles = []\n",
        "\n",
        "    # Loop through each article ID in the list\n",
        "    for id in list_of_ids:\n",
        "\n",
        "        # Use the article ID to construct a link and retrieve the HTML page\n",
        "        uClient  = uReq(make_a_link(id))\n",
        "        page_html = uClient.read()\n",
        "        page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "        # Extract the title of the article\n",
        "        Title = page_soup.find(\"h1\",{\"class\":\"heading-title\"}).text.replace(\"\\n\",\"\").strip()\n",
        "\n",
        "        # Extract the abstract of the article\n",
        "        Abstract = page_soup.find(\"div\",{\"class\":\"abstract-content\"})\n",
        "        if(Abstract is not None):\n",
        "            Abstract = Abstract.text.replace(\"\\n\",\"\").strip()\n",
        "\n",
        "        # Extract the link to the full article (if available)\n",
        "        Full_Article_link = page_soup.find(\"a\",{\"data-ga-action\":\"DOI\"})\n",
        "        if(Full_Article_link is not None):\n",
        "            Full_Article_link = page_soup.find(\"a\",{\"data-ga-action\":\"DOI\"})[\"href\"]\n",
        "            Link_full_articles.append(Full_Article_link)\n",
        "        else:\n",
        "            Link_full_articles.append(\"NA\")\n",
        "\n",
        "        # Extract the type of the article\n",
        "        Type = page_soup.find(\"span\",\"article-source\").text.strip()\n",
        "\n",
        "        # Append extracted information to respective lists\n",
        "        Article_title.append(Title)\n",
        "        Article_Abstract.append(Abstract)\n",
        "        Pub_med_link.append(make_a_link(id))\n",
        "        Type_of_articles.append(Type)\n",
        "\n",
        "    # Create a new DataFrame to store the extracted information\n",
        "    new_articles = pd.DataFrame()\n",
        "    new_articles[\"Title\"]=Article_title\n",
        "    new_articles[\"abstract\"]=Article_Abstract\n",
        "    new_articles[\"link to full article\"]=Link_full_articles\n",
        "    new_articles['Type']=Type_of_articles\n",
        "    new_articles[\"pub_med_links\"]=Pub_med_link\n",
        "    return(new_articles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St-b4WeOjYBD"
      },
      "source": [
        "# **Pre-processing - Sentence Tokenisation**: The following cell contains a method to tokenize abstracts into sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J0UmdNPoLq0o"
      },
      "outputs": [],
      "source": [
        "# Function to tokenize abstracts in a DataFrame using NLTK's sentence tokenizer\n",
        "def tokenize_abs(df):\n",
        "  # Load the English sentence tokenizer from NLTK\n",
        "  tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "  # Lists to store the IDs and tokenized sentences\n",
        "  ids = []\n",
        "  sents = []\n",
        "\n",
        "  # Loop through each row in the DataFrame\n",
        "  for x in range(len(df)):\n",
        "\n",
        "      # Get the abstract data from the current row\n",
        "      data = df.iloc[x]['abstract']\n",
        "\n",
        "      # Tokenize the abstract into sentences using NLTK's sentence tokenizer\n",
        "      sentences = tokenizer.tokenize(data)\n",
        "\n",
        "      # Append the sentences and corresponding IDs to the lists\n",
        "      for sent in sentences:\n",
        "          sents.append(sent)\n",
        "          ids.append(x)\n",
        "\n",
        "  # Create a pandas DataFrame with 'id' and 'Sentence' columns\n",
        "  sentences_df = pd.DataFrame()\n",
        "  sentences_df['id']=ids\n",
        "  sentences_df['Sentence']=sents\n",
        "  # Return the DataFrame containing sentences and IDs\n",
        "  return(sentences_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eVZdMm2m9FB8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def filter_sentences_genes(df,list_of_genes):\n",
        "  # Lists to store information about whether a sentence contains a gene and the gene itself\n",
        "  contains_ = []\n",
        "  gene = []\n",
        "\n",
        "  # Iterate through each row in the DataFrame\n",
        "  for x in range(len(df)):\n",
        "\n",
        "    # Initialize variables to track gene information for the current sentence\n",
        "    c = 0 # Flag to indicate whether the sentence contains a gene\n",
        "    g = '' # Variable to store the gene found in the sentence\n",
        "\n",
        "    # Extract the sentence from the DataFrame\n",
        "    sent = df['Sentence'].iloc[x]\n",
        "\n",
        "    # Iterate through the list of genes to check if they are present in the sentence\n",
        "    for y in range(len(list_of_genes)):\n",
        "      if (list_of_genes[y] in sent):\n",
        "        c = 1\n",
        "        g = list_of_genes[y]\n",
        "        continue\n",
        "    # Append the results for the current sentence to the lists\n",
        "    contains_.append(c)\n",
        "    gene.append(g)\n",
        "\n",
        "  # Add new columns 'Contain' and 'Gene' to the DataFrame\n",
        "  df['Contain']=contains_\n",
        "  df['Gene'] = gene\n",
        "\n",
        "  # Return the modified DataFrame\n",
        "  return(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS6htj3GjkJa"
      },
      "source": [
        "# **Pre-processing - Sentence Eliminator 1:** The following cell contains the method that eliminate sentences unrelated to gene regulation or expression. The method uses our pretrained classifier sentence_extraction_f.pth\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IrDth-IgNKpR"
      },
      "outputs": [],
      "source": [
        "#Sentence classifier 1\n",
        "def sentence_classification_1(df, max_len):\n",
        "\n",
        "  # Load the pre-trained model\n",
        "  model.load_state_dict(torch.load(sentence_classifier))\n",
        "  model.eval()\n",
        "  Sentences = df['Sentence']\n",
        "\n",
        "  # Extract sentences from the DataFrame\n",
        "  tokenizer = BertTokenizer.from_pretrained('biobert_v1.1_pubmed', do_lower_case=True)\n",
        "\n",
        "  # Tokenize sentences using BioBERT tokenizer\n",
        "  tokenized_texts = list(map(lambda t: ['[CLS]']+tokenizer.tokenize(t)+['[SEP]'] , Sentences))\n",
        "  max_len = max_len\n",
        "\n",
        "  # Pad sequences to a specified maximum length\n",
        "  input_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, tokenized_texts)),\n",
        "                            maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "  input_ids = pad_sequences(input_ids, maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  # Create attention masks\n",
        "  attention_masks = []\n",
        "  for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "  # Define batch size\n",
        "  batch_size = 8\n",
        "  validation_inputs = torch.tensor(input_ids)\n",
        "  validation_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
        "\n",
        "  # Create an iterator of our data with torch DataLoader\n",
        "  # Prepare validation data for the model\n",
        "  validation_data = TensorDataset(validation_inputs, validation_masks)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  # Lists to store predictions and logits\n",
        "  all_predictions=[]\n",
        "  all_predictions_0 = []\n",
        "  all_predictions_1= []\n",
        "  logits_abc = []\n",
        "\n",
        "  # Iterate through validation data\n",
        "  for batch in validation_dataloader:\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask = batch\n",
        "      # Telling the model not to compute or store gradients\n",
        "      with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        #label_ids = b_labels.to('cpu').numpy()\n",
        "        predicted_class = []\n",
        "        logits_abc.append(logits)\n",
        "        for x in range(len(logits)):\n",
        "          if(logits[x][0]>0):\n",
        "            abc=0\n",
        "          else:\n",
        "            abc=1\n",
        "          all_predictions.append(abc)\n",
        "        # Convert logits to probabilities and extract predictions for each class\n",
        "        logits = logits.cpu()\n",
        "        logits = (tf.nn.sigmoid(logits).numpy())\n",
        "        for x in range(len(logits)):\n",
        "          all_predictions_0.append(logits[x][0])\n",
        "          all_predictions_1.append(logits[x][1])\n",
        "\n",
        "  # Add predictions and logits to the DataFrame\n",
        "  df['predictied']=all_predictions\n",
        "  df['prediction_0']=all_predictions_0\n",
        "  df['prediction_1']=all_predictions_1\n",
        "\n",
        "  # Display information about the elimination process\n",
        "  print(\"Sentence Eliminator 1: eliminated\", len(df)-np.sum(df['predictied']), \"out of \",len(df))\n",
        "\n",
        "  # Fill NaN values in the DataFrame with empty strings\n",
        "  df = df.fillna('')\n",
        "\n",
        "  # Return the modified DataFrame\n",
        "  return(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SDDD620kH6A"
      },
      "source": [
        "# **Pre-processing: Sentence Eliminator 2** The following cell contains the method that eliminate sentences that do not mention any gene/protein entities. The method uses an NER pretrained model BERN2.0.\n",
        "\n",
        "BERN2.0 web service call may disconnect at times due to exceeding connection calls. We recommend running the program in small batches for bigger datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0gPg9MRIQ7ri"
      },
      "outputs": [],
      "source": [
        "#Sentence classifier 2\n",
        "\n",
        "# Function to extract genes from a sentence based on annotations\n",
        "def return_genes(sentence,X):\n",
        "    genes = []\n",
        "\n",
        "    # Iterate through annotations in the extracted annotations (X)\n",
        "    for x in range(len(X['annotations'])):\n",
        "        # Extract information about the annotation\n",
        "        typ = X['annotations'][x]['obj']\n",
        "        # Check if the annotation type is 'gene'\n",
        "        if(typ == \"gene\"):\n",
        "            begin = X['annotations'][x]['span']['begin']\n",
        "            end = X['annotations'][x]['span']['end']\n",
        "            # Extract the gene from the sentence based on the annotation span\n",
        "            sent = sentence\n",
        "            genes.append(sent[begin:end])\n",
        "    return(genes)\n",
        "\n",
        "# Function to check if a sentence contains multiple gene mentions\n",
        "def if_contains_genes(df):\n",
        "  target_gene = []\n",
        "  all_genes = []\n",
        "\n",
        "\n",
        "  # Iterate through each sentence in the DataFrame\n",
        "  for pos in range(len(df)):\n",
        "\n",
        "      # Query for raw annotations using the 'query_raw' function\n",
        "      RET = query_raw(df['Sentence'].iloc[pos])\n",
        "\n",
        "      # Extract genes from the sentence based on annotations\n",
        "      all_genes.append(return_genes(df['Sentence'].iloc[pos],RET))\n",
        "\n",
        "      # Count the number of gene annotations in the query result\n",
        "      objs = [RET['annotations'][x]['obj'] for x in range(len(RET['annotations']))].count('gene')\n",
        "\n",
        "      # Check if the sentence contains more than one gene mention\n",
        "      if(objs>1):\n",
        "          target_gene.append(1)\n",
        "      else:\n",
        "          target_gene.append(0)\n",
        "  # Add new columns 'gene_mention' and 'All_X' to the DataFrame\n",
        "  df['gene_mention']=target_gene\n",
        "  df['All_X']= all_genes\n",
        "\n",
        "  # Display information about the elimination process\n",
        "  print(\"Sentence Eliminator 2: eliminated\", len(df)-np.sum(df['gene_mention']), \"out of \",len(df))\n",
        "\n",
        "  # Fill NaN values in the DataFrame with empty strings\n",
        "  df = df.fillna('')\n",
        "\n",
        "  # Return the modified DataFrame\n",
        "  return(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZXCcra2ksgb"
      },
      "source": [
        "# **Relation Extraction: Named entity recognition and entity tagging.** The following cell contains the NER method to identify the gene/protein entities and tag sentences with all possible pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wuVR1OEXWcjM"
      },
      "outputs": [],
      "source": [
        "# Named Entity Recognition (NER) functions\n",
        "\n",
        "# Function to return gene pairs from a sentence based on annotations\n",
        "def return_gene_pairs(sentence,X):\n",
        "    genes = []\n",
        "\n",
        "    # Iterate through annotations in the provided data (X)\n",
        "    for x in range(len(X['annotations'])):\n",
        "        typ = X['annotations'][x]['obj']\n",
        "\n",
        "        # Check if the annotation type is 'gene'\n",
        "        if(typ == \"gene\"):\n",
        "            begin = X['annotations'][x]['span']['begin']\n",
        "            end = X['annotations'][x]['span']['end']\n",
        "            sent = sentence\n",
        "            genes.append(sent[begin:end])\n",
        "\n",
        "    # Generate pairs of gene combinations\n",
        "    gene_pairs = []\n",
        "    for row in genes:\n",
        "        for column in genes:\n",
        "            gene_pairs.append([row,column])\n",
        "\n",
        "    return(gene_pairs)\n",
        "\n",
        "\n",
        "# Function to return gene positions from a sentence based on annotations\n",
        "def return_gene_pos(sentence,X):\n",
        "    pos = []\n",
        "\n",
        "    # Iterate through annotations in the provided data (X)\n",
        "    for x in range(len(X['annotations'])):\n",
        "        typ = X['annotations'][x]['obj']\n",
        "\n",
        "        # Check if the annotation type is 'gene'\n",
        "        if(typ == \"gene\"):\n",
        "            begin = X['annotations'][x]['span']['begin']\n",
        "            end = X['annotations'][x]['span']['end']\n",
        "            sent = sentence\n",
        "            pos.append([begin,end])\n",
        "\n",
        "    # Check and modify positions if genes are part of an operon\n",
        "    pos = check_if_operon(sentence,pos)\n",
        "    return(pos)\n",
        "\n",
        "# Function to return gene position pairs from a sentence based on annotations\n",
        "def return_gene_pos_pairs(sentence,X):\n",
        "    pos = []\n",
        "    # Iterate through annotations in the provided data (X)\n",
        "    for x in range(len(X['annotations'])):\n",
        "        typ = X['annotations'][x]['obj']\n",
        "\n",
        "        # Check if the annotation type is 'gene'\n",
        "        if(typ == \"gene\"):\n",
        "            begin = X['annotations'][x]['span']['begin']\n",
        "            end = X['annotations'][x]['span']['end']\n",
        "            sent = sentence\n",
        "            pos.append([begin,end])\n",
        "\n",
        "    # Check and modify positions if genes are part of an operon\n",
        "    pos = check_if_operon(sentence,pos)\n",
        "    # Generate pairs of gene position combinations\n",
        "    gene_pos_pairs = []\n",
        "    for row in pos:\n",
        "        for column in pos:\n",
        "            gene_pos_pairs.append([row,column])\n",
        "    return(gene_pos_pairs)\n",
        "\n",
        "# Function to query raw annotations for a given text\n",
        "def query_raw(text, url=\"http://bern2.korea.ac.kr/plain\"):\n",
        "  try:\n",
        "    return requests.post(url, json={'text': text}).json()\n",
        "  except:\n",
        "    query_raw(text)\n",
        "\n",
        "# Function to check if genes are part of an operon and modify positions accordingly\n",
        "def check_if_operon(sentence,pos):\n",
        "    new_list = pos.copy()\n",
        "    pos_to_remove = []\n",
        "    # Iterate through positions to check for operons\n",
        "    for x in range(len(pos)-1):\n",
        "        this = pos[x][1]\n",
        "        next = pos[x+1][0]\n",
        "        if(next-this==1):\n",
        "            if(sentence[next-1]==\"-\"):\n",
        "                pos_to_remove.append(x)\n",
        "                pos_to_remove.append(x+1)\n",
        "\n",
        "    # Modify positions based on operons\n",
        "    if(len(pos_to_remove)>0):\n",
        "        pos_to_remove = list(dict.fromkeys(pos_to_remove))\n",
        "        pair_start = pos_to_remove[0]\n",
        "        pair_end = 0\n",
        "        for y in pos_to_remove:\n",
        "            new_list.remove(pos[y])\n",
        "        for y in range(len(pos_to_remove)):\n",
        "\n",
        "            if(y != len(pos_to_remove)-1):\n",
        "                if(pos_to_remove[y]-pos_to_remove[y+1] == -1):\n",
        "                    pair_end = 0\n",
        "                else:\n",
        "                    pair_end = pos_to_remove[y]\n",
        "                if(pair_end != 0):\n",
        "                    new_list.insert(pair_start,[pos[pair_start][0],pos[pair_end][1]])\n",
        "                    pair_end = 0\n",
        "                    pair_start = pos_to_remove[y+1]\n",
        "            else:\n",
        "                pair_end = pos_to_remove[y]\n",
        "                new_list.insert(pair_start,[pos[pair_start][0],pos[pair_end][1]])\n",
        "\n",
        "    return(new_list)\n",
        "\n",
        "# Function for Named Entity Recognition (NER) tasks\n",
        "def NER_1(df):\n",
        "  # Initialize empty lists to store information\n",
        "  ID = []\n",
        "  sent_tagged = []\n",
        "  sent_original = []\n",
        "  pair = []\n",
        "  add_lengths = 0\n",
        "  agents=[]\n",
        "  targets=[]\n",
        "  All_X = []\n",
        "  all_non_interested_genes = []\n",
        "  new_sents = []\n",
        "\n",
        "  # Extract unique sentences from the dataframe\n",
        "  sentences = list(df[\"Sentence\"].unique())\n",
        "\n",
        "  # Iterate through unique sentences\n",
        "  IDs = []\n",
        "  ind = []\n",
        "  for sentence in sentences:\n",
        "    # Extract document ID for the current sentence\n",
        "    index = list(df[df['Sentence']==sentence][\"id\"])\n",
        "    IDs.append(index[0])\n",
        "\n",
        "  # Iterate through unique sentences\n",
        "  for x in range(len(sentences)):\n",
        "      # Query raw annotations for the current sentence\n",
        "      X = query_raw(sentences[x])\n",
        "      # Get gene position pairs for the current sentence\n",
        "      pos = return_gene_pos_pairs(sentences[x],X)\n",
        "\n",
        "      # Iterate through gene position pairs\n",
        "      for j in range(len(pos)):\n",
        "          # Get gene pairs for the current sentence\n",
        "          genes = return_gene_pairs(sentences[x],X)\n",
        "          sent = sentences[x]\n",
        "\n",
        "          # Check if gene positions are different\n",
        "          if(pos[j][0] != pos[j][1]):\n",
        "              agent = genes[j][0]\n",
        "              target = genes[j][1]\n",
        "\n",
        "              # Create placeholders for agents and targets\n",
        "              agent_placeholder = \"*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!*!\"[0:len(agent)]\n",
        "              target_placeholder = \"^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#^(#\"[0:len(target)]\n",
        "\n",
        "              # Replace agent and target in the sentence with placeholders\n",
        "              sent = sent[0:pos[j][0][0]]+agent_placeholder+sent[pos[j][0][1]:len(sent)]\n",
        "              sent = sent[0:pos[j][1][0]]+target_placeholder+sent[pos[j][1][1]:len(sent)]\n",
        "              sent = sent.replace(agent_placeholder,\"GENE1\").replace(target_placeholder,\"GENE2\")\n",
        "\n",
        "          else:\n",
        "              continue\n",
        "\n",
        "          ID.append(IDs[x])\n",
        "          All_X.append(X)\n",
        "\n",
        "          agents.append(genes[j][0])\n",
        "          targets.append(genes[j][1])\n",
        "          sent_tagged.append(sent)\n",
        "\n",
        "          # Store original sentence for reference\n",
        "          sent_original.append(sentences[x])\n",
        "          ind.append(x)\n",
        "\n",
        "          # Retrieve additional gene-related details using helper functions\n",
        "          genes = return_genes(sentences[x],X)\n",
        "          pos_2 = return_gene_pos(sentences[x],X)\n",
        "          all_non_interested_genes.append(genes)\n",
        "\n",
        "          # Initialize variables for sentence modification\n",
        "          cur_sent = sent\n",
        "          temp_sent = cur_sent\n",
        "          placeholders = []\n",
        "\n",
        "          # Replace non-interested genes with placeholders\n",
        "          for y in range(len(genes)):\n",
        "\n",
        "              if(\"GENE1\" not in genes[y] and \"GENE2\" not in genes[y]):\n",
        "\n",
        "                  try:\n",
        "                    item = sent[pos_2[y][0]:pos_2[y][1]]\n",
        "                    length = len(item)\n",
        "                    placeholder = \"\"\n",
        "                    for l in range(length):\n",
        "                        placeholder = placeholder + str(y)\n",
        "                    if(len(placeholder)>0):\n",
        "                        placeholders.append(placeholder)\n",
        "                  except:\n",
        "                    continue\n",
        "\n",
        "\n",
        "                  temp_sent = temp_sent.replace(genes[y], placeholder)\n",
        "          # Replace standardized tags for consistency\n",
        "          temp_sent = temp_sent.replace(\"GENE1\",\"$GENE_AGENT#\").replace(\"GENE2\",\"$GENE_TARGET#\")\n",
        "          # Replace remaining placeholders with a common tag\n",
        "          for p in placeholders:\n",
        "              temp_sent = temp_sent.replace(p,\"BLANK\")\n",
        "          # Append the modified sentence to the list\n",
        "          new_sents.append(temp_sent)\n",
        "\n",
        "  # Create a new DataFrame to store the processed information\n",
        "  new_df = pd.DataFrame()\n",
        "  new_df[\"ID\"]=ID\n",
        "  new_df['ind']=ind\n",
        "  new_df['Sent original'] = sent_original\n",
        "  new_df['Sent tagged'] = sent_tagged\n",
        "  new_df['Agent'] = agents\n",
        "  new_df['Target'] = targets\n",
        "  new_df[\"All_X\"] =All_X\n",
        "  new_df[\"all_non_interested_genes\"]=all_non_interested_genes\n",
        "  new_df[\"new_sents_blanks\"]=new_sents\n",
        "\n",
        "  # Fill NaN values in the DataFrame with empty strings\n",
        "  new_df = new_df.fillna('')\n",
        "\n",
        "  # Return the modified DataFrame\n",
        "  return(new_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEIlPRtemKWX"
      },
      "source": [
        "# **Relation extraction- Relation classification:** The following cell contains the method for relation extraction. The model uses our fine-tunned pretrained BioBERT classifier relation_extraction_f.pth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Q-Rz3EkKJ7Yq"
      },
      "outputs": [],
      "source": [
        "# Function for relation extraction\n",
        "def relation_extraction(df):\n",
        "\n",
        "  # Load the pre-trained relation extraction model\n",
        "  model.load_state_dict(torch.load(relation_extraction_model))\n",
        "  model.cuda()\n",
        "\n",
        "  # Extract features from the DataFrame\n",
        "  df_features = df[['new_sents_blanks']]\n",
        "  feature = df_features['new_sents_blanks']\n",
        "\n",
        "\n",
        "  # Tokenize and prepare input data for the model\n",
        "  tokenizer = BertTokenizer.from_pretrained('biobert_v1.1_pubmed', do_lower_case=True)\n",
        "\n",
        "  tokenized_texts = list(map(lambda t: ['[CLS]']+tokenizer.tokenize(t)+['[SEP]'] , feature))\n",
        "  max_len = 256\n",
        "\n",
        "  input_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, tokenized_texts)),\n",
        "                              maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "  input_ids = pad_sequences(input_ids, maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  attention_masks = []\n",
        "\n",
        "  for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "  batch_size = 8\n",
        "  validation_inputs = torch.tensor(input_ids)\n",
        "  validation_masks = torch.tensor(attention_masks, dtype=torch.long)\n",
        "\n",
        "  # Create an iterator for the validation data\n",
        "  validation_data = TensorDataset(validation_inputs, validation_masks)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "  all_predictions=[]\n",
        "  all_predictions_0 = []\n",
        "  all_predictions_1= []\n",
        "  all_predictions=[]\n",
        "  logits_abc = []\n",
        "\n",
        "  # Iterate through validation data\n",
        "  for batch in validation_dataloader:\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask = batch\n",
        "      # Telling the model not to compute or store gradients\n",
        "      with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        predicted_class = []\n",
        "        logits_abc.append(logits)\n",
        "\n",
        "        # Iterate through logits to make predictions\n",
        "        for x in range(len(logits)):\n",
        "          if(logits[x][0]>0):\n",
        "            abc=0\n",
        "          else:\n",
        "            abc=1\n",
        "          all_predictions.append(abc)\n",
        "\n",
        "        # Convert logits to probabilities and extract predictions for each class\n",
        "        logits = logits.cpu()\n",
        "        logits = (tf.nn.sigmoid(logits).numpy())\n",
        "\n",
        "        for x in range(len(logits)):\n",
        "          all_predictions_0.append(logits[x][0])\n",
        "          all_predictions_1.append(logits[x][1])\n",
        "  # Add relation extraction predictions and probabilities to the DataFrame\n",
        "  df['prediction_RE']=all_predictions\n",
        "  df['prediction_RE_0']=all_predictions_0\n",
        "  df['prediction_RE_1']=all_predictions_1\n",
        "\n",
        "  # Fill NaN values in the DataFrame with empty strings\n",
        "  df = df.fillna('')\n",
        "\n",
        "  # Return the modified DataFrame\n",
        "  return(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14308YeqnEry"
      },
      "source": [
        "# **Post-processing- Refinement:** The following cell contains code to refine entity names and computing confidence factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5Y_6_VNp6kpg"
      },
      "outputs": [],
      "source": [
        "# Function to remove unwanted words from 'Agent' and 'Target' columns\n",
        "def remove_unwanted_wds(df):\n",
        "  # Read the list of unwanted words from an Excel file\n",
        "  df_uw_2 = pd.read_excel(unwanted_words_list)\n",
        "\n",
        "  # Initialize lists to store refined 'Agent' and 'Target' values\n",
        "  agent_refined = []\n",
        "  target_refined = []\n",
        "\n",
        "  # Iterate through the DataFrame rows\n",
        "  for x in range(len(df)):\n",
        "\n",
        "      # Process 'Agent' column\n",
        "      wrd = df['Agent'].iloc[x]\n",
        "      if(len(df['Agent'].iloc[x].split(\" \"))!=1):\n",
        "          wrd = \"\"\n",
        "          for y in df['Agent'].iloc[x].split(\" \"):\n",
        "              if(y not in list(df_uw_2['Unwanted_words'])):\n",
        "                wrd =wrd + y + \" \"\n",
        "      wrd = wrd.strip()\n",
        "      agent_refined.append(wrd)\n",
        "\n",
        "  for x in range(len(df)):\n",
        "\n",
        "      # Process 'Target' column\n",
        "      wrd = df['Target'].iloc[x]\n",
        "      if(len(df['Target'].iloc[x].split(\" \"))!=1):\n",
        "          wrd = \"\"\n",
        "          for y in df['Target'].iloc[x].split(\" \"):\n",
        "              if(y not in list(df_uw_2['Unwanted_words'])):\n",
        "                wrd =wrd + y + \" \"\n",
        "      wrd = wrd.strip()\n",
        "      target_refined.append(wrd)\n",
        "\n",
        "  # Add refined 'Agent' and 'Target' columns to the DataFrame\n",
        "  df['Agent_uws']=agent_refined\n",
        "  df['Target_uws']=target_refined\n",
        "\n",
        "  # Fill any NaN values in the DataFrame with empty strings\n",
        "  df = df.fillna('')\n",
        "\n",
        "  # Return the modified DataFrame\n",
        "  return(df)\n",
        "\n",
        "# Function to remove non-gene words from 'Agent_uws' and 'Target_uws' columns\n",
        "def remove_non_gene(df):\n",
        "\n",
        "  # Initialize lists to store refined 'Agent' and 'Target' values\n",
        "  Refined_Agents = []\n",
        "\n",
        "  # Iterate through the DataFrame rows\n",
        "  for y in range(len(df)):\n",
        "      # Process 'Agent_uws' column\n",
        "      temp = df['Agent_uws'].iloc[y]\n",
        "\n",
        "      temp = temp.split(\" \")\n",
        "      gene_name = ''\n",
        "      if(len(temp)>1):\n",
        "          for x in range(len(temp)):\n",
        "              if(len(temp[x])>0 and temp[x]!=\" \"):\n",
        "                  X = query_raw(temp[x])\n",
        "                  if('annotations' in list(X.keys())):\n",
        "                    if(len(X['annotations'])>0):\n",
        "                        if (X['annotations'][0]['obj'] == \"gene\"):\n",
        "                            gene_name = gene_name + temp[x] + \" \"\n",
        "          gene_name = gene_name.strip()\n",
        "          Refined_Agents.append(gene_name)\n",
        "      else:\n",
        "          Refined_Agents.append(temp[0])\n",
        "  Refined_Target = []\n",
        "  for y in range(len(df)):\n",
        "      # Process 'Target_uws' column\n",
        "      temp = df['Target_uws'].iloc[y]\n",
        "      temp = temp.split(\" \")\n",
        "      gene_name = ''\n",
        "      if(len(temp)>1):\n",
        "          for x in range(len(temp)):\n",
        "              if(len(temp[x])>0 and temp[x]!=\" \"):\n",
        "                  X = query_raw(temp[x])\n",
        "                  if('annotations' in list(X.keys())):\n",
        "                    if(len(X['annotations'])>0):\n",
        "                        if (X['annotations'][0]['obj'] == \"gene\"):\n",
        "                            gene_name = gene_name + temp[x] + \" \"\n",
        "          gene_name = gene_name.strip()\n",
        "          Refined_Target.append(gene_name)\n",
        "      else:\n",
        "          Refined_Target.append(temp[0])\n",
        "\n",
        "  # Add refined 'Agent_uws_gn' and 'Target_uws_gn' columns to the DataFrame\n",
        "  df['Agent_uws_gn']=Refined_Agents\n",
        "  df['Target_uws_gn']=Refined_Target\n",
        "\n",
        "  # Fill any NaN values in the DataFrame with empty strings\n",
        "  df = df.fillna('')\n",
        "\n",
        "  # Return the modified DataFrame\n",
        "  return(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27no6n8gonl8"
      },
      "source": [
        "# **Post-processing: Confidence computation:** The following cell contains the methods for computing confidence factor for extracted relations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EW9llrK1Mkb5"
      },
      "outputs": [],
      "source": [
        "# Function to compute confidence scores at the relation level\n",
        "def compute_confidence(df2):\n",
        "  confidence = []\n",
        "  unique_sentences = []\n",
        "\n",
        "  # Iterate through relation pairs\n",
        "  for pair in df2['Relation']:\n",
        "      score = 0\n",
        "      abc = df2[df2['Relation']==pair][['ID','Sent original','prediction_RE_1']]\n",
        "      unique_sentences.append(len(abc[\"Sent original\"].unique()))\n",
        "\n",
        "      # Compute confidence score for each unique sentence\n",
        "      for x in abc[\"Sent original\"].unique():\n",
        "          score = score + np.average(abc[abc['Sent original']==x]['prediction_RE_1'])\n",
        "      confidence.append(score)\n",
        "\n",
        "  # Add 'sentence_level_c' and 're_unique_sentences' columns to the DataFrame\n",
        "  df2['sentence_level_c'] = confidence\n",
        "  df2['re_unique_sentences']=unique_sentences #sentences giving the same relationship\n",
        "  return df2\n",
        "\n",
        "# Function to check if relations are related to E. Coli regulations\n",
        "def if_e_coli_reg(df2):\n",
        "  df = pd.read_excel(E_Coli_Regulations)\n",
        "  rel_exists = []\n",
        "\n",
        "  # Iterate through DataFrame rows\n",
        "  for x in range(len(df2)):\n",
        "      agent = df2['Agent_uws_gn'].iloc[x]\n",
        "      target = df2['Target_uws_gn'].iloc[x]\n",
        "      rele = 0\n",
        "\n",
        "      # Iterate through E. Coli regulations\n",
        "      for y in range(len(df)):\n",
        "          if(df['Agent'].iloc[y].lower() in agent.lower()):\n",
        "              if(df['Target'].iloc[y].lower() in target.lower()):\n",
        "                  rele = 1\n",
        "                  break\n",
        "\n",
        "      rel_exists.append(rele)\n",
        "\n",
        "  # Add 'E_Coli_regulation' column to the DataFrame\n",
        "  df2['E_Coli_regulation']=rel_exists\n",
        "  return df2\n",
        "\n",
        "\n",
        "# Function to check if agents are related to E. Coli regulons\n",
        "\n",
        "def if_e_coli_regulon(df2):\n",
        "  df = pd.read_excel(E_Coli_Regulations)\n",
        "  E_Coli_regulons = []\n",
        "  for x in list(df['Agent'].unique()):\n",
        "      E_Coli_regulons.append(x.lower())\n",
        "  is_regulon = []\n",
        "\n",
        "  # Iterate through DataFrame agents\n",
        "  for agent in df2['Agent_uws_gn']:\n",
        "      is_r = 0\n",
        "      # Iterate through E. Coli regulons\n",
        "      for ag in E_Coli_regulons:\n",
        "          if(ag.lower() in agent.lower().split(\" \")):\n",
        "              is_r=1\n",
        "\n",
        "              break\n",
        "      is_regulon.append(is_r)\n",
        "\n",
        "  # Add 'is_regulon' column to the DataFrame\n",
        "  df2['is_regulon']=is_regulon\n",
        "  return df2\n",
        "\n",
        "\n",
        "# Function to calculate the final confidence score\n",
        "def final_conf(df2):\n",
        "\n",
        "  # Add 'c' column to the DataFrame\n",
        "  df2['c'] = df2['sentence_level_c'] + if_e_coli_regulon(df2)['is_regulon'] + 3*if_e_coli_reg(df2)['E_Coli_regulation']\n",
        "  return df2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Search for articles based on keywords\n",
        "E_Coli = search(Keywords,Article_count)\n",
        "\n",
        "# Extract and preprocess articles\n",
        "df_articles = extract_articles(E_Coli['IdList'])\n",
        "df_articles = df_articles.dropna(axis=0)\n",
        "\n",
        "# Tokenize abstracts and perform sentence classification\n",
        "df_sents_1 = tokenize_abs(df_articles)\n",
        "df_sents_2 = sentence_classification_1(df_sents_1, max_len)\n",
        "df_sents_2 = df_sents_2.fillna('')\n",
        "\n",
        "# Identify sentences containing genes\n",
        "df_sents_3 = if_contains_genes(df_sents_2[df_sents_2[\"predictied\"]==1])\n",
        "\n",
        "# Perform Named Entity Recognition (NER)\n",
        "df_sents_NER = NER_1(df_sents_3[df_sents_3[\"gene_mention\"]==1])\n",
        "\n",
        "# Extract relations using relation extraction model\n",
        "df_sents_RE = relation_extraction(df_sents_NER)\n",
        "\n",
        "# Remove unwanted words from refined relations\n",
        "df_RE_refined_1 = remove_unwanted_wds(df_sents_RE[df_sents_RE['prediction_RE']==1])\n",
        "df_RE_refined_1 = df_RE_refined_1.fillna('')\n",
        "\n",
        "# Remove non-gene words from refined relations\n",
        "df_RE_refined_2 = remove_non_gene(df_RE_refined_1)\n",
        "df_RE_refined_2 = df_RE_refined_2.fillna('')\n",
        "\n",
        "# Filter out rows where 'Agent_uws_gn' and 'Target_uws_gn' are not present\n",
        "df_RE_refined_2 = df_RE_refined_2[df_RE_refined_2['Agent_uws_gn']!=\"\"]\n",
        "df_RE_refined_2 = df_RE_refined_2[df_RE_refined_2['Target_uws_gn']!=\"\"]\n",
        "\n",
        "# Create a 'Relation' column by combining 'Agent_uws_gn' and 'Target_uws_gn'\n",
        "df_RE_refined_2['Relation'] = df_RE_refined_2['Agent_uws_gn']+\"--\"+df_RE_refined_2['Target_uws_gn']\n",
        "\n",
        "# Compute confidence scores for relations\n",
        "df_RE_scored = compute_confidence(df_RE_refined_2)\n",
        "# For E. Coli regulations\n",
        "df_RE_scored_EColi = final_conf(df_RE_scored)\n",
        "\n",
        "# Extract final relations and remove duplicates\n",
        "Final_relations = df_RE_scored_EColi[['Agent_uws_gn','Target_uws_gn','Relation','sentence_level_c','re_unique_sentences',\t'is_regulon',\t'E_Coli_regulation',\t'c']].drop_duplicates()\n",
        "\n",
        "# Sort and export the final relations to an Excel file\n",
        "print(\"Final Relations extracted as as follows\")\n",
        "Final_relations.sort_values(['c'],ascending=False).to_excel(\"ExGI_result.xlsx\")\n",
        "Final_relations.sort_values(['c'],ascending=False)"
      ],
      "metadata": {
        "id": "8qPJcbIoYphy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "T4"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
